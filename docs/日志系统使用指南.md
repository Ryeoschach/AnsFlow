# AnsFlowæ—¥å¿—ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

AnsFlowç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿå·²å®ŒæˆPhase 2å¼€å‘ï¼Œæä¾›ï¼š
- ç»Ÿä¸€çš„Django/FastAPIæ—¥å¿—ç®¡ç†
- Redis Streamså®æ—¶æ—¥å¿—ç¼“å†²
- WebSocketå®æ—¶æ—¥å¿—æ¨é€
- ç»“æ„åŒ–JSONæ—¥å¿—æ ¼å¼
- è‡ªåŠ¨æ•æ„Ÿæ•°æ®è„±æ•
- ä¼ä¸šçº§æ—¥å¿—è½®è½¬å’Œå­˜å‚¨

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€æ—¥å¿—è®°å½•

```python
import logging
from common.logging_config import get_logger, log_with_context

# è·å–æ—¥å¿—å™¨
logger = get_logger('ansflow.mymodule')

# åŸºç¡€æ—¥å¿—è®°å½•
logger.info('ç”¨æˆ·ç™»å½•æˆåŠŸ')
logger.warning('ç¼“å­˜è¿æ¥è¾ƒæ…¢')
logger.error('APIè°ƒç”¨å¤±è´¥')

# å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—è®°å½•
log_with_context(
    logger, 
    'INFO', 
    'è®¢å•åˆ›å»ºæˆåŠŸ',
    request=request,
    extra={'order_id': 12345, 'amount': 99.99},
    labels=['order', 'payment']
)
```

### 2. Redisæ—¥å¿—æµä½¿ç”¨

```python
import asyncio
from common.redis_logging import RedisLogStreams

# åˆå§‹åŒ–Redisæ—¥å¿—æµ
redis_logger = RedisLogStreams()

# è¿æ¥Redis
if redis_logger.connect():
    # å¼‚æ­¥å†™å…¥æ—¥å¿—
    await redis_logger.log_async(
        level='INFO',
        message='ç³»ç»Ÿäº‹ä»¶è®°å½•',
        service='django_service',
        user_id=123,
        action='create_pipeline'
    )
    
    # è¯»å–æœ€è¿‘æ—¥å¿—
    recent_logs = redis_logger.read_logs_stream(count=50)
    print(f"è·å–åˆ° {len(recent_logs)} æ¡æ—¥å¿—")
```

### 3. WebSocketå®æ—¶æ—¥å¿—

```javascript
// å‰ç«¯JavaScript
import { RealTimeLogClient } from './services/realTimeLogClient';

const logClient = new RealTimeLogClient('ws://localhost:8000/ws/logs/');

// ç›‘å¬æ—¥å¿—æ¶ˆæ¯
logClient.on('log', (logData) => {
    console.log('æ–°æ—¥å¿—:', logData);
    // æ›´æ–°UIæ˜¾ç¤º
});

// ç›‘å¬è¿æ¥çŠ¶æ€
logClient.on('connectionStatus', (status) => {
    console.log('è¿æ¥çŠ¶æ€:', status);
});

// è®¾ç½®æ—¥å¿—è¿‡æ»¤å™¨
logClient.setFilter({
    levels: ['ERROR', 'WARNING', 'INFO'],
    services: ['django_service'],
    keywords: ['ç”¨æˆ·', 'è®¢å•']
});

// è¿æ¥WebSocket
logClient.connect();
```

### 4. ç¼“å†²æ—¥å¿—ç®¡ç†

```python
from common.websocket_logging import BufferedLogManager, LogFilter

# åˆ›å»ºç¼“å†²ç®¡ç†å™¨
buffer_manager = BufferedLogManager(buffer_size=1000)

# æ·»åŠ æ—¥å¿—åˆ°ç¼“å†²åŒº
buffer_manager.add_log({
    'level': 'INFO',
    'message': 'ç”¨æˆ·æ“ä½œè®°å½•',
    'timestamp': '2025-08-05T06:40:00Z',
    'user_id': 123
})

# è·å–è¿‡æ»¤åçš„æ—¥å¿—
log_filter = LogFilter(
    levels=['ERROR', 'WARNING'],
    services=['django_service'],
    keywords=['é”™è¯¯', 'å¼‚å¸¸']
)

filtered_logs = buffer_manager.get_recent_logs(count=100, log_filter=log_filter)
```

## é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .envæ–‡ä»¶
LOG_LEVEL=INFO
LOG_DIR=/path/to/logs
LOG_FORMAT=json
LOGGING_ENABLE_REDIS=true
LOGGING_ENABLE_WEBSOCKET=true
LOG_ROTATION=daily
LOG_RETENTION_DAYS=30

# Redisé…ç½®
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_LOG_DB=1
```

### Djangoè®¾ç½®

```python
# settings/base.py
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'ansflow_json': {
            '()': 'common.logging_config.AnsFlowJSONFormatter',
        },
    },
    'handlers': {
        'console': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'ansflow_json',
        },
        'file': {
            'level': 'INFO',
            'class': 'logging.handlers.TimedRotatingFileHandler',
            'filename': BASE_DIR / 'logs' / 'django.log',
            'formatter': 'ansflow_json',
        },
    },
    'loggers': {
        'ansflow': {
            'handlers': ['console', 'file'],
            'level': 'DEBUG',
            'propagate': False,
        },
    },
}
```

## æ—¥å¿—æ ¼å¼

### æ ‡å‡†æ—¥å¿—ç»“æ„

```json
{
    "timestamp": "2025-08-05T06:40:00.123Z",
    "level": "INFO",
    "service": "django_service",
    "module": "ansflow.pipelines",
    "message": "æµæ°´çº¿æ‰§è¡Œå®Œæˆ",
    "trace_id": "req_abc12345",
    "user_id": 123,
    "user_name": "admin",
    "ip": "192.168.1.100",
    "method": "POST",
    "path": "/api/pipelines/execute",
    "status_code": 200,
    "response_time_ms": 1250,
    "labels": ["pipeline", "execution", "success"],
    "extra": {
        "pipeline_id": 456,
        "steps_count": 5,
        "execution_time": "00:02:30"
    }
}
```

### æ•æ„Ÿæ•°æ®è„±æ•

ç³»ç»Ÿè‡ªåŠ¨è„±æ•ä»¥ä¸‹å­—æ®µï¼š
- password, passwd, pwd
- token, key, secret
- authorization, cookie, session
- csrf

```json
{
    "user_data": {
        "username": "admin",
        "password": "adm*******",  // è‡ªåŠ¨è„±æ•
        "token": "******"          // è‡ªåŠ¨è„±æ•
    }
}
```

## æ€§èƒ½ä¼˜åŒ–

### Redisé…ç½®å»ºè®®

```python
# Redisè¿æ¥æ± é…ç½®
REDIS_LOG_CONFIG = {
    'host': 'localhost',
    'port': 6379,
    'db': 1,
    'max_connections': 20,
    'retry_on_timeout': True,
    'health_check_interval': 30
}

# æ—¥å¿—æµé…ç½®
LOG_STREAM_CONFIG = {
    'stream_name': 'ansflow:logs',
    'max_len': 10000,         # æµæœ€å¤§é•¿åº¦
    'batch_size': 100,        # æ‰¹å¤„ç†å¤§å°
    'retention_hours': 24     # æ•°æ®ä¿ç•™æ—¶é—´
}
```

### æ–‡ä»¶æ—¥å¿—è½®è½¬

```python
# æŒ‰çº§åˆ«åˆ†ç¦»æ—¥å¿—æ–‡ä»¶
LOGGING_FILE_CONFIG = {
    'error_log': 'logs/error.log',      # åªè®°å½•ERROR
    'warning_log': 'logs/warning.log',  # åªè®°å½•WARNING
    'info_log': 'logs/info.log',        # åªè®°å½•INFO
    'debug_log': 'logs/debug.log'       # åªè®°å½•DEBUG
}

# è½®è½¬ç­–ç•¥
LOG_ROTATION = {
    'when': 'midnight',    # æ¯å¤©åˆå¤œè½®è½¬
    'interval': 1,         # è½®è½¬é—´éš”
    'backupCount': 30,     # ä¿ç•™30å¤©
    'encoding': 'utf-8'
}
```

## ç›‘æ§å’Œå‘Šè­¦

### æ—¥å¿—çº§åˆ«ç›‘æ§

```python
# é”™è¯¯çº§åˆ«è‡ªåŠ¨å‘Šè­¦
ERROR_ALERT_CONFIG = {
    'error_threshold': 10,      # 5åˆ†é’Ÿå†…è¶…è¿‡10ä¸ªERROR
    'warning_threshold': 50,    # 5åˆ†é’Ÿå†…è¶…è¿‡50ä¸ªWARNING
    'time_window': 300,         # æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
}

# é›†æˆPrometheusæŒ‡æ ‡
from django_prometheus import metrics

error_counter = metrics.Counter(
    'ansflow_logs_error_total',
    'Total number of error logs',
    labelnames=['service', 'module']
)
```

### å¥åº·æ£€æŸ¥

```python
# æ—¥å¿—ç³»ç»Ÿå¥åº·æ£€æŸ¥
def check_logging_health():
    checks = {
        'redis_connection': False,
        'file_write': False,
        'websocket_ready': False
    }
    
    # æ£€æŸ¥Redisè¿æ¥
    try:
        redis_logger = RedisLogStreams()
        checks['redis_connection'] = redis_logger.connect()
    except Exception:
        pass
    
    # æ£€æŸ¥æ–‡ä»¶å†™å…¥
    try:
        test_logger = get_logger('health_check')
        test_logger.info('Health check test')
        checks['file_write'] = True
    except Exception:
        pass
    
    return checks
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Redisè¿æ¥å¤±è´¥**
   - æ£€æŸ¥RedisæœåŠ¡çŠ¶æ€
   - éªŒè¯è¿æ¥é…ç½®
   - æŸ¥çœ‹é˜²ç«å¢™è®¾ç½®

2. **æ—¥å¿—æ–‡ä»¶æƒé™é—®é¢˜**
   - ç¡®ä¿æ—¥å¿—ç›®å½•å†™æƒé™
   - æ£€æŸ¥ç£ç›˜ç©ºé—´
   - éªŒè¯æ–‡ä»¶è·¯å¾„

3. **WebSocketè¿æ¥æ–­å¼€**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - éªŒè¯è®¤è¯çŠ¶æ€
   - æŸ¥çœ‹æœåŠ¡å™¨è´Ÿè½½

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger('common.redis_logging').setLevel(logging.DEBUG)
logging.getLogger('common.websocket_logging').setLevel(logging.DEBUG)

# æµ‹è¯•æ—¥å¿—ç³»ç»Ÿ
def test_logging_system():
    from common.logging_config import AnsFlowLoggingConfig
    from common.redis_logging import RedisLogStreams
    from common.websocket_logging import BufferedLogManager
    
    print("ğŸš€ å¼€å§‹æ—¥å¿—ç³»ç»Ÿæµ‹è¯•...")
    
    # æµ‹è¯•é…ç½®
    config = AnsFlowLoggingConfig()
    print(f"âœ… é…ç½®åŠ è½½: {config.log_dir}")
    
    # æµ‹è¯•Redis
    redis_logger = RedisLogStreams()
    if redis_logger.connect():
        print("âœ… Redisè¿æ¥æˆåŠŸ")
    else:
        print("âŒ Redisè¿æ¥å¤±è´¥")
    
    # æµ‹è¯•ç¼“å†²ç®¡ç†å™¨
    buffer_manager = BufferedLogManager()
    buffer_manager.add_log({'test': 'data'})
    print(f"âœ… ç¼“å†²ç®¡ç†å™¨: {len(buffer_manager.get_recent_logs())} æ¡æ—¥å¿—")
    
    print("ğŸ¯ æµ‹è¯•å®Œæˆ")
```

## æœ€ä½³å®è·µ

### 1. æ—¥å¿—çº§åˆ«ä½¿ç”¨

- **DEBUG**: è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼Œä»…å¼€å‘ç¯å¢ƒä½¿ç”¨
- **INFO**: ä¸€èˆ¬ä¿¡æ¯ï¼Œä¸šåŠ¡æµç¨‹å…³é”®èŠ‚ç‚¹
- **WARNING**: è­¦å‘Šä¿¡æ¯ï¼Œéœ€è¦å…³æ³¨ä½†ä¸å½±å“è¿è¡Œ
- **ERROR**: é”™è¯¯ä¿¡æ¯ï¼Œéœ€è¦ç«‹å³å¤„ç†

### 2. ä¸Šä¸‹æ–‡ä¿¡æ¯

å§‹ç»ˆåŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
```python
log_with_context(
    logger, 'INFO', 'è®¢å•å¤„ç†å®Œæˆ',
    request=request,
    extra={
        'order_id': order.id,
        'user_id': order.user_id,
        'amount': order.amount,
        'processing_time': processing_time
    },
    labels=['order', 'payment', 'success']
)
```

### 3. å¼‚æ­¥æ—¥å¿—ä½¿ç”¨

å¯¹äºé«˜é¢‘ç‡çš„æ—¥å¿—å†™å…¥ï¼Œä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼š
```python
# é«˜æ€§èƒ½å¼‚æ­¥æ—¥å¿—
async def process_batch_orders(orders):
    for order in orders:
        await redis_logger.log_async(
            level='INFO',
            message=f'å¤„ç†è®¢å• {order.id}',
            service='order_service',
            order_id=order.id
        )
```

### 4. é”™è¯¯å¤„ç†

å§‹ç»ˆåŒ…å«å¼‚å¸¸å¤„ç†çš„æ—¥å¿—ï¼š
```python
try:
    result = process_payment(order)
    logger.info('æ”¯ä»˜å¤„ç†æˆåŠŸ', extra={'order_id': order.id, 'amount': order.amount})
except PaymentError as e:
    logger.error('æ”¯ä»˜å¤„ç†å¤±è´¥', extra={
        'order_id': order.id,
        'error_code': e.code,
        'error_message': str(e)
    })
    raise
```

## æ€»ç»“

AnsFlowæ—¥å¿—ç³»ç»ŸPhase 2å·²å®Œæˆæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å¼€å‘å’Œæµ‹è¯•ï¼Œç³»ç»Ÿç¨³å®šå¯é ï¼Œå¯æŠ•å…¥ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ—¥å¿—ç®¡ç†è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒé«˜æ€§èƒ½ã€å®æ—¶æ€§å’Œå¯è§‚æµ‹æ€§è¦æ±‚ã€‚

å¦‚æœ‰é—®é¢˜æˆ–éœ€è¦æŠ€æœ¯æ”¯æŒï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤ç« èŠ‚æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚

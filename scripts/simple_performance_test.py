#!/usr/bin/env python3
"""
AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰
åœ¨ä¸ä¾èµ–å¤–éƒ¨åº“çš„æƒ…å†µä¸‹æµ‹è¯•å¹¶è¡Œç»„æ‰§è¡Œæ€§èƒ½
"""

import asyncio
import time
import json
import sys
import os
import statistics
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime

@dataclass
class SimplePerformanceMetrics:
    """ç®€åŒ–çš„æ€§èƒ½æŒ‡æ ‡"""
    execution_time: float
    parallel_groups_count: int
    total_steps_count: int
    max_parallel_workers: int
    success_rate: float
    avg_step_time: float
    max_step_time: float

class SimplePerformanceTester:
    """ç®€åŒ–çš„æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results: List[SimplePerformanceMetrics] = []
        
    def create_test_pipeline(self, num_groups: int, steps_per_group: int) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„æµæ°´çº¿é…ç½®"""
        steps = []
        parallel_groups = []
        
        for group_idx in range(num_groups):
            group_id = f"test_group_{group_idx}"
            group = {
                'id': group_id,
                'name': f'å¹¶è¡Œç»„ {group_idx + 1}',
                'sync_policy': 'wait_all',
                'order': group_idx + 1,
                'timeout_seconds': 300
            }
            parallel_groups.append(group)
            
            for step_idx in range(steps_per_group):
                step_id = f"step_{group_idx}_{step_idx}"
                step = {
                    'id': step_id,
                    'name': f'æµ‹è¯•æ­¥éª¤ {group_idx}-{step_idx}',
                    'type': 'test',
                    'order': group_idx + 1,
                    'parallel_group': group_id,
                    'estimated_duration': 0.1 + (step_idx * 0.05)
                }
                steps.append(step)
        
        return {
            'steps': steps,
            'parallel_groups': parallel_groups,
            'total_groups': num_groups,
            'total_steps': len(steps)
        }
    
    async def simulate_parallel_execution(self, pipeline_config: Dict[str, Any], 
                                        max_workers: int) -> SimplePerformanceMetrics:
        """æ¨¡æ‹Ÿå¹¶è¡Œæ‰§è¡Œ"""
        start_time = time.time()
        
        steps = pipeline_config['steps']
        parallel_groups = pipeline_config['parallel_groups']
        
        step_execution_times = []
        successful_steps = 0
        
        try:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                for group in parallel_groups:
                    group_steps = [s for s in steps if s.get('parallel_group') == group['id']]
                    
                    futures = []
                    for step in group_steps:
                        future = executor.submit(self._simulate_step_execution, step)
                        futures.append(future)
                    
                    for future in as_completed(futures):
                        try:
                            step_time = future.result(timeout=30)
                            step_execution_times.append(step_time)
                            successful_steps += 1
                        except Exception as e:
                            print(f"æ­¥éª¤æ‰§è¡Œå¤±è´¥: {e}")
        
        except Exception as e:
            print(f"å¹¶è¡Œæ‰§è¡Œå¤±è´¥: {e}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        success_rate = successful_steps / len(steps) if steps else 0
        avg_step_time = statistics.mean(step_execution_times) if step_execution_times else 0
        max_step_time = max(step_execution_times) if step_execution_times else 0
        
        return SimplePerformanceMetrics(
            execution_time=total_time,
            parallel_groups_count=len(parallel_groups),
            total_steps_count=len(steps),
            max_parallel_workers=max_workers,
            success_rate=success_rate,
            avg_step_time=avg_step_time,
            max_step_time=max_step_time
        )
    
    def _simulate_step_execution(self, step: Dict[str, Any]) -> float:
        """æ¨¡æ‹Ÿå•ä¸ªæ­¥éª¤çš„æ‰§è¡Œ"""
        start_time = time.time()
        
        execution_time = step.get('estimated_duration', 0.1)
        time.sleep(execution_time)
        
        # æ¨¡æ‹ŸCPUæ“ä½œ
        result = 0
        for i in range(1000):
            result += i * i
        
        return time.time() - start_time
    
    async def run_comprehensive_test(self) -> List[SimplePerformanceMetrics]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        print("=" * 60)
        print("AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰")
        print("=" * 60)
        
        test_configurations = [
            # (å¹¶è¡Œç»„æ•°, æ¯ç»„æ­¥éª¤æ•°, å·¥ä½œçº¿ç¨‹æ•°)
            (1, 3, 4),
            (2, 4, 8),
            (3, 5, 8),
            (5, 3, 12),
            (8, 4, 16),
            (10, 3, 20),
        ]
        
        test_results = []
        
        for groups_count, steps_per_group, max_workers in test_configurations:
            print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {groups_count}ä¸ªå¹¶è¡Œç»„, {steps_per_group}æ­¥éª¤/ç»„, {max_workers}ä¸ªå·¥ä½œçº¿ç¨‹")
            
            iteration_results = []
            for iteration in range(3):  # æ¯ä¸ªé…ç½®æµ‹è¯•3æ¬¡
                print(f"  è¿­ä»£ {iteration + 1}/3", end=" ")
                
                pipeline_config = self.create_test_pipeline(groups_count, steps_per_group)
                
                try:
                    metrics = await asyncio.wait_for(
                        self.simulate_parallel_execution(pipeline_config, max_workers),
                        timeout=60
                    )
                    iteration_results.append(metrics)
                    
                    print(f"âœ… {metrics.execution_time:.2f}s ({metrics.success_rate:.1%})")
                    
                except asyncio.TimeoutError:
                    print("âŒ è¶…æ—¶")
                except Exception as e:
                    print(f"âŒ å¤±è´¥: {e}")
            
            # è®¡ç®—å¹³å‡å€¼
            if iteration_results:
                avg_metrics = SimplePerformanceMetrics(
                    execution_time=statistics.mean([m.execution_time for m in iteration_results]),
                    parallel_groups_count=groups_count,
                    total_steps_count=groups_count * steps_per_group,
                    max_parallel_workers=max_workers,
                    success_rate=statistics.mean([m.success_rate for m in iteration_results]),
                    avg_step_time=statistics.mean([m.avg_step_time for m in iteration_results]),
                    max_step_time=max([m.max_step_time for m in iteration_results])
                )
                test_results.append(avg_metrics)
                
                print(f"  ğŸ“Š å¹³å‡æ€§èƒ½: {avg_metrics.execution_time:.2f}s, {avg_metrics.success_rate:.1%}æˆåŠŸç‡")
        
        self.test_results = test_results
        return test_results
    
    def analyze_results(self) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not self.test_results:
            return {'error': 'æ²¡æœ‰æµ‹è¯•ç»“æœ'}
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        execution_times = [r.execution_time for r in self.test_results]
        success_rates = [r.success_rate for r in self.test_results]
        
        # æ‰¾åˆ°æœ€ä½³é…ç½®
        best_time_result = min(self.test_results, key=lambda x: x.execution_time)
        best_success_result = max(self.test_results, key=lambda x: x.success_rate)
        
        # å¯æ‰©å±•æ€§åˆ†æ
        scalability_score = self._calculate_scalability_score()
        
        return {
            'performance_summary': {
                'avg_execution_time': statistics.mean(execution_times),
                'min_execution_time': min(execution_times),
                'max_execution_time': max(execution_times),
                'avg_success_rate': statistics.mean(success_rates),
                'min_success_rate': min(success_rates)
            },
            'best_configurations': {
                'fastest': {
                    'groups': best_time_result.parallel_groups_count,
                    'workers': best_time_result.max_parallel_workers,
                    'time': best_time_result.execution_time
                },
                'most_reliable': {
                    'groups': best_success_result.parallel_groups_count,
                    'workers': best_success_result.max_parallel_workers,
                    'success_rate': best_success_result.success_rate
                }
            },
            'scalability_score': scalability_score,
            'recommendations': self._generate_recommendations()
        }
    
    def _calculate_scalability_score(self) -> float:
        """è®¡ç®—å¯æ‰©å±•æ€§è¯„åˆ†"""
        if len(self.test_results) < 2:
            return 50.0
        
        # åŸºäºæ‰§è¡Œæ—¶é—´å¢é•¿ç‡è®¡ç®—
        execution_times = [r.execution_time for r in self.test_results]
        groups_counts = [r.parallel_groups_count for r in self.test_results]
        
        # è®¡ç®—æ—¶é—´ä¸ç»„æ•°çš„ç›¸å…³æ€§
        time_variance = statistics.variance(execution_times)
        groups_variance = statistics.variance(groups_counts)
        
        # ç†æƒ³æƒ…å†µä¸‹ï¼Œæ—¶é—´å¢é•¿åº”è¯¥ä¸ç»„æ•°æˆæ¯”ä¾‹
        # è¯„åˆ†åŸºäºæ—¶é—´ç¨³å®šæ€§å’ŒæˆåŠŸç‡
        avg_success_rate = statistics.mean([r.success_rate for r in self.test_results])
        
        base_score = avg_success_rate * 50  # æˆåŠŸç‡è´¡çŒ®50åˆ†
        stability_score = max(0, 50 - time_variance * 10)  # ç¨³å®šæ€§è´¡çŒ®50åˆ†
        
        return min(100.0, base_score + stability_score)
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if not self.test_results:
            return ["æ— æ³•ç”Ÿæˆå»ºè®®ï¼šç¼ºå°‘æµ‹è¯•æ•°æ®"]
        
        # æ‰§è¡Œæ—¶é—´åˆ†æ
        max_time = max([r.execution_time for r in self.test_results])
        if max_time > 10:
            recommendations.append(
                f"â±ï¸ æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼ˆæœ€å¤§ {max_time:.1f}sï¼‰ï¼šå»ºè®®ä¼˜åŒ–æ­¥éª¤å¹¶è¡Œåº¦æˆ–å¢åŠ å·¥ä½œçº¿ç¨‹"
            )
        
        # æˆåŠŸç‡åˆ†æ
        min_success_rate = min([r.success_rate for r in self.test_results])
        if min_success_rate < 0.9:
            recommendations.append(
                f"âŒ æ‰§è¡ŒæˆåŠŸç‡åä½ï¼ˆæœ€ä½ {min_success_rate:.1%}ï¼‰ï¼šå»ºè®®å¢åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"
            )
        
        # å·¥ä½œçº¿ç¨‹ä¼˜åŒ–
        worker_performance = {}
        for result in self.test_results:
            workers = result.max_parallel_workers
            if workers not in worker_performance:
                worker_performance[workers] = []
            worker_performance[workers].append(result.execution_time)
        
        if len(worker_performance) > 1:
            best_workers = min(worker_performance.keys(), 
                             key=lambda w: statistics.mean(worker_performance[w]))
            recommendations.append(
                f"ğŸš€ æœ€ä½³å·¥ä½œçº¿ç¨‹æ•°é…ç½®ï¼šæ¨èä½¿ç”¨ {best_workers} ä¸ªå¹¶è¡Œå·¥ä½œçº¿ç¨‹"
            )
        
        # å¯æ‰©å±•æ€§å»ºè®®
        scalability_score = self._calculate_scalability_score()
        if scalability_score < 60:
            recommendations.append(
                "ğŸ“ˆ å¯æ‰©å±•æ€§å¾…æ”¹è¿›ï¼šè€ƒè™‘å®æ–½æ‰¹å¤„ç†å’Œèµ„æºæ± ç®¡ç†"
            )
        
        return recommendations
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            return "æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ"
        
        analysis = self.analyze_results()
        
        report = []
        report.append("=" * 70)
        report.append("AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 70)
        report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(self.test_results)}")
        report.append("")
        
        # æ€§èƒ½æ¦‚è§ˆ
        if 'performance_summary' in analysis:
            summary = analysis['performance_summary']
            report.append("ğŸ“Š æ€§èƒ½æ¦‚è§ˆ")
            report.append("-" * 40)
            report.append(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {summary['avg_execution_time']:.2f}s")
            report.append(f"æœ€å¿«æ‰§è¡Œæ—¶é—´: {summary['min_execution_time']:.2f}s")
            report.append(f"æœ€æ…¢æ‰§è¡Œæ—¶é—´: {summary['max_execution_time']:.2f}s")
            report.append(f"å¹³å‡æˆåŠŸç‡: {summary['avg_success_rate']:.1%}")
            report.append(f"å¯æ‰©å±•æ€§è¯„åˆ†: {analysis['scalability_score']:.1f}/100")
            report.append("")
        
        # æœ€ä½³é…ç½®
        if 'best_configurations' in analysis:
            best = analysis['best_configurations']
            report.append("ğŸ† æœ€ä½³é…ç½®")
            report.append("-" * 40)
            report.append(f"æœ€å¿«é…ç½®: {best['fastest']['groups']}ç»„/{best['fastest']['workers']}çº¿ç¨‹ "
                         f"({best['fastest']['time']:.2f}s)")
            report.append(f"æœ€å¯é é…ç½®: {best['most_reliable']['groups']}ç»„/"
                         f"{best['most_reliable']['workers']}çº¿ç¨‹ "
                         f"({best['most_reliable']['success_rate']:.1%})")
            report.append("")
        
        # ä¼˜åŒ–å»ºè®®
        if 'recommendations' in analysis:
            report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
            report.append("-" * 40)
            for recommendation in analysis['recommendations']:
                report.append(f"  â€¢ {recommendation}")
            report.append("")
        
        # è¯¦ç»†ç»“æœ
        report.append("ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ")
        report.append("-" * 40)
        report.append(f"{'ç»„æ•°':<6} {'æ­¥éª¤æ•°':<8} {'çº¿ç¨‹æ•°':<8} {'æ‰§è¡Œæ—¶é—´':<10} {'æˆåŠŸç‡':<8} {'å¹³å‡æ­¥éª¤æ—¶é—´':<12}")
        report.append("-" * 65)
        
        for result in self.test_results:
            steps_per_group = result.total_steps_count // result.parallel_groups_count
            report.append(f"{result.parallel_groups_count:<6} "
                         f"{steps_per_group:<8} "
                         f"{result.max_parallel_workers:<8} "
                         f"{result.execution_time:<10.2f} "
                         f"{result.success_rate:<8.1%} "
                         f"{result.avg_step_time:<12.3f}")
        
        report.append("")
        report.append("=" * 70)
        
        return "\n".join(report)

async def main():
    """ä¸»å‡½æ•°"""
    tester = SimplePerformanceTester()
    
    print("å¯åŠ¨ AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
    
    start_time = time.time()
    await tester.run_comprehensive_test()
    total_time = time.time() - start_time
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.1f}s")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = tester.generate_report()
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    report_filename = f"ansflow_simple_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    script_dir = os.path.dirname(os.path.abspath(__file__))
    report_path = os.path.join(script_dir, report_filename)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    print("\n" + report)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâŒ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

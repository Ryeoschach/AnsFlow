#!/usr/bin/env python3
"""
AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½ä¼˜åŒ–å’Œå‹åŠ›æµ‹è¯•
æµ‹è¯•å¤§è§„æ¨¡å¹¶è¡Œç»„åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import asyncio
import time
import json
import sys
import os
import statistics
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta
import psutil
import gc

# æ·»åŠ Djangoè·¯å¾„
sys.path.append('/Users/creed/Workspace/OpenSource/ansflow/backend/django_service')

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    parallel_groups_count: int
    total_steps_count: int
    max_parallel_workers: int
    success_rate: float
    avg_step_time: float
    max_step_time: float
    min_step_time: float

@dataclass
class StressTestConfig:
    """å‹åŠ›æµ‹è¯•é…ç½®"""
    parallel_groups_counts: List[int]
    steps_per_group_range: tuple
    max_workers_range: List[int]
    test_iterations: int
    timeout_seconds: int

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.test_results: List[PerformanceMetrics] = []
        self.optimization_recommendations: List[str] = []
        
    def create_test_pipeline(self, num_groups: int, steps_per_group: int) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„æµæ°´çº¿é…ç½®"""
        steps = []
        parallel_groups = []
        
        # åˆ›å»ºå¹¶è¡Œç»„
        for group_idx in range(num_groups):
            group_id = f"test_group_{group_idx}"
            group = {
                'id': group_id,
                'name': f'å¹¶è¡Œç»„ {group_idx + 1}',
                'sync_policy': 'wait_all',
                'order': group_idx + 1,
                'timeout_seconds': 300
            }
            parallel_groups.append(group)
            
            # ä¸ºæ¯ä¸ªç»„åˆ›å»ºæ­¥éª¤
            for step_idx in range(steps_per_group):
                step_id = f"step_{group_idx}_{step_idx}"
                step = {
                    'id': step_id,
                    'name': f'æµ‹è¯•æ­¥éª¤ {group_idx}-{step_idx}',
                    'type': 'test',
                    'order': group_idx + 1,
                    'parallel_group': group_id,
                    'parameters': {
                        'test_command': f'echo "æ‰§è¡Œæ­¥éª¤ {step_id}"; sleep {0.1 + (step_idx * 0.05)}; echo "å®Œæˆ"'
                    },
                    'estimated_duration': 0.1 + (step_idx * 0.05)
                }
                steps.append(step)
        
        return {
            'steps': steps,
            'parallel_groups': parallel_groups,
            'total_groups': num_groups,
            'total_steps': len(steps)
        }
    
    def monitor_system_resources(self) -> Dict[str, float]:
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        
        return {
            'memory_mb': process.memory_info().rss / 1024 / 1024,
            'cpu_percent': process.cpu_percent(),
            'system_memory_percent': psutil.virtual_memory().percent,
            'system_cpu_percent': psutil.cpu_percent(interval=0.1)
        }
    
    async def simulate_parallel_execution(self, pipeline_config: Dict[str, Any], 
                                        max_workers: int) -> PerformanceMetrics:
        """æ¨¡æ‹Ÿå¹¶è¡Œæ‰§è¡Œå¹¶æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        start_time = time.time()
        start_resources = self.monitor_system_resources()
        
        steps = pipeline_config['steps']
        parallel_groups = pipeline_config['parallel_groups']
        
        step_execution_times = []
        successful_steps = 0
        
        try:
            # ä½¿ç”¨ThreadPoolExecutoræ¨¡æ‹Ÿå¹¶è¡Œæ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æŒ‰ç»„å¹¶è¡Œæ‰§è¡Œ
                for group in parallel_groups:
                    group_steps = [s for s in steps if s.get('parallel_group') == group['id']]
                    
                    # æäº¤ç»„å†…æ‰€æœ‰æ­¥éª¤åˆ°çº¿ç¨‹æ± 
                    futures = []
                    for step in group_steps:
                        future = executor.submit(self._simulate_step_execution, step)
                        futures.append(future)
                    
                    # ç­‰å¾…ç»„å†…æ‰€æœ‰æ­¥éª¤å®Œæˆ
                    for future in as_completed(futures):
                        try:
                            step_time = future.result(timeout=30)
                            step_execution_times.append(step_time)
                            successful_steps += 1
                        except Exception as e:
                            print(f"æ­¥éª¤æ‰§è¡Œå¤±è´¥: {e}")
        
        except Exception as e:
            print(f"å¹¶è¡Œæ‰§è¡Œå¤±è´¥: {e}")
        
        end_time = time.time()
        end_resources = self.monitor_system_resources()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time = end_time - start_time
        avg_memory = (start_resources['memory_mb'] + end_resources['memory_mb']) / 2
        avg_cpu = (start_resources['cpu_percent'] + end_resources['cpu_percent']) / 2
        
        success_rate = successful_steps / len(steps) if steps else 0
        avg_step_time = statistics.mean(step_execution_times) if step_execution_times else 0
        max_step_time = max(step_execution_times) if step_execution_times else 0
        min_step_time = min(step_execution_times) if step_execution_times else 0
        
        return PerformanceMetrics(
            execution_time=total_time,
            memory_usage_mb=avg_memory,
            cpu_usage_percent=avg_cpu,
            parallel_groups_count=len(parallel_groups),
            total_steps_count=len(steps),
            max_parallel_workers=max_workers,
            success_rate=success_rate,
            avg_step_time=avg_step_time,
            max_step_time=max_step_time,
            min_step_time=min_step_time
        )
    
    def _simulate_step_execution(self, step: Dict[str, Any]) -> float:
        """æ¨¡æ‹Ÿå•ä¸ªæ­¥éª¤çš„æ‰§è¡Œ"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
        execution_time = step.get('estimated_duration', 0.1)
        time.sleep(execution_time)
        
        # æ¨¡æ‹Ÿä¸€äº›CPUå¯†é›†å‹æ“ä½œ
        result = 0
        for i in range(1000):
            result += i * i
        
        return time.time() - start_time
    
    async def run_stress_test(self, config: StressTestConfig) -> List[PerformanceMetrics]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        print("=" * 60)
        print("AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½å‹åŠ›æµ‹è¯•")
        print("=" * 60)
        
        test_results = []
        
        for groups_count in config.parallel_groups_counts:
            for max_workers in config.max_workers_range:
                print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {groups_count}ä¸ªå¹¶è¡Œç»„, {max_workers}ä¸ªå·¥ä½œçº¿ç¨‹")
                
                iteration_results = []
                
                for iteration in range(config.test_iterations):
                    print(f"  è¿­ä»£ {iteration + 1}/{config.test_iterations}")
                    
                    # ç”Ÿæˆéšæœºæ­¥éª¤æ•°
                    import random
                    steps_per_group = random.randint(*config.steps_per_group_range)
                    
                    # åˆ›å»ºæµ‹è¯•æµæ°´çº¿
                    pipeline_config = self.create_test_pipeline(groups_count, steps_per_group)
                    
                    # æ‰§è¡Œæµ‹è¯•
                    try:
                        metrics = await asyncio.wait_for(
                            self.simulate_parallel_execution(pipeline_config, max_workers),
                            timeout=config.timeout_seconds
                        )
                        iteration_results.append(metrics)
                        
                        print(f"    âœ… æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}s, "
                              f"æˆåŠŸç‡: {metrics.success_rate:.1%}, "
                              f"å†…å­˜: {metrics.memory_usage_mb:.1f}MB")
                        
                    except asyncio.TimeoutError:
                        print(f"    âŒ æµ‹è¯•è¶…æ—¶ ({config.timeout_seconds}s)")
                    except Exception as e:
                        print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                    await asyncio.sleep(0.1)  # çŸ­æš‚ä¼‘æ¯
                
                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                if iteration_results:
                    avg_metrics = self._calculate_average_metrics(iteration_results)
                    test_results.append(avg_metrics)
                    
                    print(f"  ğŸ“Š å¹³å‡æ€§èƒ½: {avg_metrics.execution_time:.2f}s, "
                          f"{avg_metrics.memory_usage_mb:.1f}MB, "
                          f"{avg_metrics.success_rate:.1%} æˆåŠŸç‡")
        
        self.test_results.extend(test_results)
        return test_results
    
    def _calculate_average_metrics(self, metrics_list: List[PerformanceMetrics]) -> PerformanceMetrics:
        """è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡"""
        if not metrics_list:
            return PerformanceMetrics(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
        
        return PerformanceMetrics(
            execution_time=statistics.mean([m.execution_time for m in metrics_list]),
            memory_usage_mb=statistics.mean([m.memory_usage_mb for m in metrics_list]),
            cpu_usage_percent=statistics.mean([m.cpu_usage_percent for m in metrics_list]),
            parallel_groups_count=metrics_list[0].parallel_groups_count,
            total_steps_count=metrics_list[0].total_steps_count,
            max_parallel_workers=metrics_list[0].max_parallel_workers,
            success_rate=statistics.mean([m.success_rate for m in metrics_list]),
            avg_step_time=statistics.mean([m.avg_step_time for m in metrics_list]),
            max_step_time=max([m.max_step_time for m in metrics_list]),
            min_step_time=min([m.min_step_time for m in metrics_list])
        )
    
    def analyze_performance_trends(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿å¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        if not self.test_results:
            return {'error': 'æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä¾›åˆ†æ'}
        
        # æŒ‰å¹¶è¡Œç»„æ•°é‡åˆ†ç»„åˆ†æ
        groups_performance = {}
        for result in self.test_results:
            key = f"{result.parallel_groups_count}groups"
            if key not in groups_performance:
                groups_performance[key] = []
            groups_performance[key].append(result)
        
        # åˆ†ææ€§èƒ½ç“¶é¢ˆ
        analysis = {
            'performance_trends': {},
            'bottleneck_analysis': {},
            'optimization_recommendations': []
        }
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        for group_key, metrics_list in groups_performance.items():
            execution_times = [m.execution_time for m in metrics_list]
            memory_usage = [m.memory_usage_mb for m in metrics_list]
            success_rates = [m.success_rate for m in metrics_list]
            
            analysis['performance_trends'][group_key] = {
                'avg_execution_time': statistics.mean(execution_times),
                'execution_time_variance': statistics.variance(execution_times) if len(execution_times) > 1 else 0,
                'avg_memory_usage': statistics.mean(memory_usage),
                'avg_success_rate': statistics.mean(success_rates),
                'scalability_score': self._calculate_scalability_score(metrics_list)
            }
        
        # ç“¶é¢ˆåˆ†æ
        max_memory_result = max(self.test_results, key=lambda x: x.memory_usage_mb)
        max_time_result = max(self.test_results, key=lambda x: x.execution_time)
        min_success_result = min(self.test_results, key=lambda x: x.success_rate)
        
        analysis['bottleneck_analysis'] = {
            'memory_bottleneck': {
                'max_memory_mb': max_memory_result.memory_usage_mb,
                'config': f"{max_memory_result.parallel_groups_count}ç»„/{max_memory_result.max_parallel_workers}çº¿ç¨‹"
            },
            'time_bottleneck': {
                'max_time_s': max_time_result.execution_time,
                'config': f"{max_time_result.parallel_groups_count}ç»„/{max_time_result.max_parallel_workers}çº¿ç¨‹"
            },
            'reliability_bottleneck': {
                'min_success_rate': min_success_result.success_rate,
                'config': f"{min_success_result.parallel_groups_count}ç»„/{min_success_result.max_parallel_workers}çº¿ç¨‹"
            }
        }
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        analysis['optimization_recommendations'] = self._generate_optimization_recommendations()
        
        return analysis
    
    def _calculate_scalability_score(self, metrics_list: List[PerformanceMetrics]) -> float:
        """è®¡ç®—å¯æ‰©å±•æ€§è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        if len(metrics_list) < 2:
            return 50.0
        
        # åŸºäºæ‰§è¡Œæ—¶é—´å¢é•¿ç‡å’ŒæˆåŠŸç‡ç¨³å®šæ€§è®¡ç®—
        execution_times = [m.execution_time for m in metrics_list]
        success_rates = [m.success_rate for m in metrics_list]
        
        # æ—¶é—´å¢é•¿ç‡æƒ©ç½š
        time_growth_penalty = statistics.variance(execution_times) * 10
        
        # æˆåŠŸç‡ç¨³å®šæ€§å¥–åŠ±
        success_stability_bonus = statistics.mean(success_rates) * 50
        
        # åŸºç¡€åˆ†æ•°
        base_score = 50.0
        
        score = base_score + success_stability_bonus - time_growth_penalty
        return max(0.0, min(100.0, score))
    
    def _generate_optimization_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if not self.test_results:
            return ["æ— æ³•ç”Ÿæˆå»ºè®®ï¼šç¼ºå°‘æµ‹è¯•æ•°æ®"]
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        avg_memory = statistics.mean([r.memory_usage_mb for r in self.test_results])
        if avg_memory > 500:
            recommendations.append(
                f"ğŸ”¥ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼ˆå¹³å‡ {avg_memory:.1f}MBï¼‰ï¼š"
                "\n  - å®æ–½æ­¥éª¤ç»“æœç¼“å­˜æœºåˆ¶"
                "\n  - ä¼˜åŒ–å¹¶è¡Œç»„æ‰¹å¤„ç†å¤§å°"
                "\n  - è€ƒè™‘åˆ†æ‰¹æ‰§è¡Œå¤§å‹å¹¶è¡Œç»„"
            )
        
        # æ‰§è¡Œæ—¶é—´åˆ†æ
        max_time = max([r.execution_time for r in self.test_results])
        if max_time > 30:
            recommendations.append(
                f"â±ï¸ æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼ˆæœ€å¤§ {max_time:.1f}sï¼‰ï¼š"
                "\n  - å¢åŠ å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°"
                "\n  - ä¼˜åŒ–æ­¥éª¤æ‰§è¡Œç®—æ³•"
                "\n  - å®æ–½å¼‚æ­¥æ‰§è¡Œæ¨¡å¼"
            )
        
        # æˆåŠŸç‡åˆ†æ
        min_success_rate = min([r.success_rate for r in self.test_results])
        if min_success_rate < 0.9:
            recommendations.append(
                f"âŒ æ‰§è¡ŒæˆåŠŸç‡åä½ï¼ˆæœ€ä½ {min_success_rate:.1%}ï¼‰ï¼š"
                "\n  - å¢åŠ é”™è¯¯é‡è¯•æœºåˆ¶"
                "\n  - å®æ–½ç†”æ–­å™¨æ¨¡å¼"
                "\n  - ä¼˜åŒ–èµ„æºç«äº‰å¤„ç†"
            )
        
        # å¯æ‰©å±•æ€§åˆ†æ
        group_counts = [r.parallel_groups_count for r in self.test_results]
        if max(group_counts) > 10:
            recommendations.append(
                "ğŸ“ˆ å¤§è§„æ¨¡å¹¶è¡Œç»„ä¼˜åŒ–å»ºè®®ï¼š"
                "\n  - å®æ–½åˆ†å±‚å¹¶è¡Œè°ƒåº¦"
                "\n  - ä½¿ç”¨é˜Ÿåˆ—ç¼“å†²æœºåˆ¶"
                "\n  - è€ƒè™‘åˆ†å¸ƒå¼æ‰§è¡Œæ¶æ„"
            )
        
        # å·¥ä½œçº¿ç¨‹ä¼˜åŒ–
        worker_performance = {}
        for result in self.test_results:
            workers = result.max_parallel_workers
            if workers not in worker_performance:
                worker_performance[workers] = []
            worker_performance[workers].append(result.execution_time)
        
        if len(worker_performance) > 1:
            best_workers = min(worker_performance.keys(), 
                             key=lambda w: statistics.mean(worker_performance[w]))
            recommendations.append(
                f"ğŸš€ æœ€ä½³å·¥ä½œçº¿ç¨‹æ•°é…ç½®ï¼š"
                f"\n  - æ¨èä½¿ç”¨ {best_workers} ä¸ªå¹¶è¡Œå·¥ä½œçº¿ç¨‹"
                f"\n  - å¹³å‡æ‰§è¡Œæ—¶é—´: {statistics.mean(worker_performance[best_workers]):.2f}s"
            )
        
        return recommendations
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            return "æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ"
        
        analysis = self.analyze_performance_trends()
        
        report = []
        report.append("=" * 80)
        report.append("AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(self.test_results)}")
        report.append("")
        
        # æ€»ä½“æ€§èƒ½æ¦‚è§ˆ
        report.append("ğŸ“Š æ€»ä½“æ€§èƒ½æ¦‚è§ˆ")
        report.append("-" * 40)
        avg_exec_time = statistics.mean([r.execution_time for r in self.test_results])
        avg_memory = statistics.mean([r.memory_usage_mb for r in self.test_results])
        avg_success_rate = statistics.mean([r.success_rate for r in self.test_results])
        
        report.append(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_exec_time:.2f}s")
        report.append(f"å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.1f}MB")
        report.append(f"å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")
        report.append("")
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        if 'performance_trends' in analysis:
            report.append("ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ")
            report.append("-" * 40)
            for group_key, trend in analysis['performance_trends'].items():
                report.append(f"{group_key}:")
                report.append(f"  æ‰§è¡Œæ—¶é—´: {trend['avg_execution_time']:.2f}s")
                report.append(f"  å†…å­˜ä½¿ç”¨: {trend['avg_memory_usage']:.1f}MB")
                report.append(f"  æˆåŠŸç‡: {trend['avg_success_rate']:.1%}")
                report.append(f"  å¯æ‰©å±•æ€§è¯„åˆ†: {trend['scalability_score']:.1f}/100")
                report.append("")
        
        # ç“¶é¢ˆåˆ†æ
        if 'bottleneck_analysis' in analysis:
            report.append("ğŸ” ç“¶é¢ˆåˆ†æ")
            report.append("-" * 40)
            bottlenecks = analysis['bottleneck_analysis']
            
            report.append(f"å†…å­˜ç“¶é¢ˆ: {bottlenecks['memory_bottleneck']['max_memory_mb']:.1f}MB "
                         f"({bottlenecks['memory_bottleneck']['config']})")
            report.append(f"æ—¶é—´ç“¶é¢ˆ: {bottlenecks['time_bottleneck']['max_time_s']:.2f}s "
                         f"({bottlenecks['time_bottleneck']['config']})")
            report.append(f"å¯é æ€§ç“¶é¢ˆ: {bottlenecks['reliability_bottleneck']['min_success_rate']:.1%} "
                         f"({bottlenecks['reliability_bottleneck']['config']})")
            report.append("")
        
        # ä¼˜åŒ–å»ºè®®
        if 'optimization_recommendations' in analysis:
            report.append("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®")
            report.append("-" * 40)
            for recommendation in analysis['optimization_recommendations']:
                report.append(recommendation)
                report.append("")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        report.append("ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ")
        report.append("-" * 40)
        report.append(f"{'ç»„æ•°':<6} {'çº¿ç¨‹':<6} {'æ‰§è¡Œæ—¶é—´':<10} {'å†…å­˜MB':<8} {'æˆåŠŸç‡':<8} {'å¹³å‡æ­¥éª¤æ—¶é—´':<12}")
        report.append("-" * 70)
        
        for result in self.test_results:
            report.append(f"{result.parallel_groups_count:<6} "
                         f"{result.max_parallel_workers:<6} "
                         f"{result.execution_time:<10.2f} "
                         f"{result.memory_usage_mb:<8.1f} "
                         f"{result.success_rate:<8.1%} "
                         f"{result.avg_step_time:<12.3f}")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)

async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‹åŠ›æµ‹è¯•å‚æ•°
    stress_config = StressTestConfig(
        parallel_groups_counts=[1, 3, 5, 8, 10, 15, 20],  # ä¸åŒçš„å¹¶è¡Œç»„æ•°é‡
        steps_per_group_range=(2, 8),  # æ¯ç»„æ­¥éª¤æ•°èŒƒå›´
        max_workers_range=[4, 8, 12, 16],  # ä¸åŒçš„å·¥ä½œçº¿ç¨‹æ•°
        test_iterations=3,  # æ¯ç§é…ç½®çš„è¿­ä»£æ¬¡æ•°
        timeout_seconds=60  # å•æ¬¡æµ‹è¯•è¶…æ—¶æ—¶é—´
    )
    
    # åˆ›å»ºæ€§èƒ½ä¼˜åŒ–å™¨
    optimizer = PerformanceOptimizer()
    
    print("å¯åŠ¨ AnsFlow å¹¶è¡Œæ‰§è¡Œæ€§èƒ½ä¼˜åŒ–æµ‹è¯•...")
    print(f"ç³»ç»Ÿä¿¡æ¯: CPUæ ¸å¿ƒæ•°={psutil.cpu_count()}, å†…å­˜={psutil.virtual_memory().total/1024/1024/1024:.1f}GB")
    print("")
    
    # è¿è¡Œå‹åŠ›æµ‹è¯•
    start_time = time.time()
    await optimizer.run_stress_test(stress_config)
    total_time = time.time() - start_time
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.1f}s")
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    report = optimizer.generate_performance_report()
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    report_filename = f"ansflow_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    script_dir = os.path.dirname(os.path.abspath(__file__))
    report_path = os.path.join(script_dir, report_filename)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    print("\n" + "=" * 80)
    print(report)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâŒ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

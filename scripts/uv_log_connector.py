#!/usr/bin/env python3
"""
AnsFlow ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ uv è¿æ¥ç®¡ç†å™¨
ç®¡ç†æ‰€æœ‰æ—¥å¿—ç›¸å…³çš„è¿æ¥å’Œä¾èµ–ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
"""

import subprocess
import sys
import os
from pathlib import Path


class UVLogConnector:
    """åŸºäºUVçš„ç»Ÿä¸€æ—¥å¿—è¿æ¥ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.services = {
            'django': self.project_root / 'backend/django_service',
            'fastapi': self.project_root / 'backend/fastapi_service'
        }
        
    def install_dependencies(self, service: str = 'all'):
        """ä½¿ç”¨uvå®‰è£…æ—¥å¿—ç›¸å…³ä¾èµ–"""
        dependencies = {
            'common': [
                'redis[hiredis]',
                'aiomysql',
                'elasticsearch[async]',
                'structlog',
                'python-json-logger'
            ],
            'django': [
                'django-redis',
                'celery[redis]',
                'django-structlog'
            ],
            'fastapi': [
                'fastapi',
                'uvicorn',
                'websockets',
                'aiofiles'
            ]
        }
        
        if service == 'all':
            for svc in self.services.keys():
                self._install_for_service(svc, dependencies)
        else:
            self._install_for_service(service, dependencies)
    
    def _install_for_service(self, service: str, dependencies: dict):
        """ä¸ºç‰¹å®šæœåŠ¡å®‰è£…ä¾èµ–"""
        if service not in self.services:
            print(f"âŒ æœªçŸ¥æœåŠ¡: {service}")
            return
        
        service_path = self.services[service]
        
        print(f"ğŸ“¦ ä¸º {service} æœåŠ¡å®‰è£…æ—¥å¿—ä¾èµ–...")
        
        # å®‰è£…é€šç”¨ä¾èµ–
        for dep in dependencies['common']:
            self._run_uv_add(service_path, dep)
        
        # å®‰è£…æœåŠ¡ç‰¹å®šä¾èµ–
        if service in dependencies:
            for dep in dependencies[service]:
                self._run_uv_add(service_path, dep)
    
    def _run_uv_add(self, service_path: Path, dependency: str):
        """è¿è¡Œuv addå‘½ä»¤"""
        try:
            result = subprocess.run([
                'uv', 'add', dependency
            ], cwd=service_path, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… {dependency} å®‰è£…æˆåŠŸ")
            else:
                print(f"âŒ {dependency} å®‰è£…å¤±è´¥: {result.stderr}")
        except FileNotFoundError:
            print("âŒ uvå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£…uv")
            sys.exit(1)
    
    def setup_unified_logging(self):
        """è®¾ç½®ç»Ÿä¸€æ—¥å¿—é…ç½®"""
        print("ğŸ”§ è®¾ç½®ç»Ÿä¸€æ—¥å¿—é…ç½®...")
        
        # åˆ›å»ºç»Ÿä¸€æ—¥å¿—é…ç½®
        config_content = '''
# AnsFlow ç»Ÿä¸€æ—¥å¿—é…ç½®
import os
from pathlib import Path

# æ—¥å¿—æ ¹ç›®å½•
LOG_ROOT = Path(os.getenv('ANSFLOW_LOG_ROOT', '/Users/creed/Workspace/OpenSource/ansflow/logs'))

# Redisé…ç½®
REDIS_CONFIG = {
    'host': os.getenv('REDIS_HOST', 'localhost'),
    'port': int(os.getenv('REDIS_PORT', '6379')),
    'db': int(os.getenv('REDIS_LOG_DB', '5')),
    'stream_name': 'ansflow:logs:stream'
}

# MySQLé…ç½®
MYSQL_CONFIG = {
    'host': os.getenv('MYSQL_HOST', 'localhost'),
    'port': int(os.getenv('MYSQL_PORT', '3306')),
    'user': os.getenv('MYSQL_USER', 'root'),
    'password': os.getenv('MYSQL_PASSWORD', ''),
    'database': os.getenv('MYSQL_DB', 'ansflow')
}

# ElasticSearché…ç½®
ELASTICSEARCH_CONFIG = {
    'enabled': os.getenv('ES_ENABLED', 'false').lower() == 'true',
    'hosts': [os.getenv('ES_HOST', 'localhost:9200')],
    'index_pattern': 'ansflow-logs-{date}'
}

# æ—¥å¿—çº§åˆ«é…ç½®
LOG_LEVELS = {
    'DEBUG': 10,
    'INFO': 20,
    'WARNING': 30,
    'ERROR': 40,
    'CRITICAL': 50
}

# æœåŠ¡é…ç½®
SERVICES = {
    'django': {
        'port': 8000,
        'log_dir': LOG_ROOT / 'services/django',
        'component': 'web'
    },
    'fastapi': {
        'port': 8001,
        'log_dir': LOG_ROOT / 'services/fastapi',
        'component': 'api'
    },
    'celery': {
        'log_dir': LOG_ROOT / 'services/celery',
        'component': 'worker'
    }
}
'''
        
        config_file = self.project_root / 'common/log_config.py'
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        print(f"âœ… ç»Ÿä¸€æ—¥å¿—é…ç½®å·²åˆ›å»º: {config_file}")
    
    def create_connection_script(self):
        """åˆ›å»ºè¿æ¥æµ‹è¯•è„šæœ¬"""
        script_content = '''#!/usr/bin/env python3
"""
AnsFlow ç»Ÿä¸€æ—¥å¿—è¿æ¥æµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰æ—¥å¿—å­˜å‚¨ç³»ç»Ÿçš„è¿æ¥çŠ¶æ€
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

try:
    import redis.asyncio as redis
    import aiomysql
    from elasticsearch import AsyncElasticsearch
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
    print("è¯·è¿è¡Œ: uv sync å®‰è£…æ‰€æœ‰ä¾èµ–")
    sys.exit(1)

from common.log_config import REDIS_CONFIG, MYSQL_CONFIG, ELASTICSEARCH_CONFIG


async def test_redis_connection():
    """æµ‹è¯•Redisè¿æ¥"""
    print("ğŸ” æµ‹è¯•Redisè¿æ¥...")
    try:
        client = redis.Redis(**REDIS_CONFIG, decode_responses=True)
        
        # æµ‹è¯•ping
        pong = await client.ping()
        if pong:
            print("âœ… Redisè¿æ¥æ­£å¸¸")
            
            # æµ‹è¯•Streamæ“ä½œ
            test_data = {
                'timestamp': datetime.now().isoformat(),
                'level': 'INFO',
                'service': 'test',
                'message': 'Redisè¿æ¥æµ‹è¯•'
            }
            
            msg_id = await client.xadd(REDIS_CONFIG['stream_name'], test_data)
            print(f"âœ… Redis Streamå†™å…¥æˆåŠŸ: {msg_id}")
            
            # è¯»å–æµ‹è¯•
            messages = await client.xread({REDIS_CONFIG['stream_name']: '$'}, count=1, block=100)
            if messages:
                print("âœ… Redis Streamè¯»å–æ­£å¸¸")
            
            await client.aclose()
            return True
        else:
            print("âŒ Redis pingå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
        return False


async def test_mysql_connection():
    """æµ‹è¯•MySQLè¿æ¥"""
    print("ğŸ” æµ‹è¯•MySQLè¿æ¥...")
    try:
        pool = await aiomysql.create_pool(
            host=MYSQL_CONFIG['host'],
            port=MYSQL_CONFIG['port'],
            user=MYSQL_CONFIG['user'],
            password=MYSQL_CONFIG['password'],
            db=MYSQL_CONFIG['database'],
            minsize=1,
            maxsize=1
        )
        
        async with pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT 1")
                result = await cursor.fetchone()
                if result and result[0] == 1:
                    print("âœ… MySQLè¿æ¥æ­£å¸¸")
                    
                    # æµ‹è¯•ç»Ÿä¸€æ—¥å¿—è¡¨æ˜¯å¦å­˜åœ¨
                    await cursor.execute("""
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = %s AND table_name = 'unified_logs'
                    """, (MYSQL_CONFIG['database'],))
                    
                    table_exists = await cursor.fetchone()
                    if table_exists and table_exists[0] > 0:
                        print("âœ… ç»Ÿä¸€æ—¥å¿—è¡¨å·²å­˜åœ¨")
                    else:
                        print("âš ï¸  ç»Ÿä¸€æ—¥å¿—è¡¨ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»º")
                        return False
        
        pool.close()
        await pool.wait_closed()
        return True
        
    except Exception as e:
        print(f"âŒ MySQLè¿æ¥å¤±è´¥: {e}")
        return False


async def test_elasticsearch_connection():
    """æµ‹è¯•ElasticSearchè¿æ¥"""
    if not ELASTICSEARCH_CONFIG['enabled']:
        print("â­ï¸  ElasticSearchæœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return True
        
    print("ğŸ” æµ‹è¯•ElasticSearchè¿æ¥...")
    try:
        es = AsyncElasticsearch(ELASTICSEARCH_CONFIG['hosts'])
        
        # æµ‹è¯•é›†ç¾¤å¥åº·çŠ¶æ€
        health = await es.cluster.health()
        if health['status'] in ['green', 'yellow']:
            print(f"âœ… ElasticSearchè¿æ¥æ­£å¸¸ (çŠ¶æ€: {health['status']})")
            
            # æµ‹è¯•ç´¢å¼•æ“ä½œ
            test_doc = {
                'timestamp': datetime.now(),
                'level': 'INFO',
                'service': 'test',
                'message': 'ElasticSearchè¿æ¥æµ‹è¯•'
            }
            
            await es.index(
                index='ansflow-logs-test',
                body=test_doc
            )
            print("âœ… ElasticSearchç´¢å¼•å†™å…¥æˆåŠŸ")
            
            await es.close()
            return True
        else:
            print(f"âŒ ElasticSearchçŠ¶æ€å¼‚å¸¸: {health['status']}")
            return False
            
    except Exception as e:
        print(f"âŒ ElasticSearchè¿æ¥å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹AnsFlowç»Ÿä¸€æ—¥å¿—è¿æ¥æµ‹è¯•...")
    print("=" * 50)
    
    results = {
        'redis': await test_redis_connection(),
        'mysql': await test_mysql_connection(),
        'elasticsearch': await test_elasticsearch_connection()
    }
    
    print("=" * 50)
    print("ğŸ“Š è¿æ¥æµ‹è¯•ç»“æœ:")
    
    all_ok = True
    for service, status in results.items():
        icon = "âœ…" if status else "âŒ"
        print(f"  {icon} {service.upper()}: {'æ­£å¸¸' if status else 'å¼‚å¸¸'}")
        if not status:
            all_ok = False
    
    if all_ok:
        print("\\nğŸ‰ æ‰€æœ‰è¿æ¥æµ‹è¯•é€šè¿‡ï¼ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿå¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
        return 0
    else:
        print("\\nâš ï¸  éƒ¨åˆ†è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æœåŠ¡çŠ¶æ€ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
'''
        
        script_file = self.project_root / 'scripts/test_log_connections.py'
        script_file.parent.mkdir(exist_ok=True)
        
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_file, 0o755)
        
        print(f"âœ… è¿æ¥æµ‹è¯•è„šæœ¬å·²åˆ›å»º: {script_file}")
    
    def create_database_schema(self):
        """åˆ›å»ºç»Ÿä¸€æ—¥å¿—æ•°æ®åº“æ¶æ„"""
        schema_sql = '''-- AnsFlow ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿæ•°æ®åº“æ¶æ„

-- åˆ›å»ºç»Ÿä¸€æ—¥å¿—è¡¨
CREATE TABLE IF NOT EXISTS unified_logs (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    log_id VARCHAR(255) UNIQUE NOT NULL COMMENT 'æ—¥å¿—å”¯ä¸€æ ‡è¯†',
    timestamp DATETIME(6) NOT NULL COMMENT 'æ—¥å¿—æ—¶é—´æˆ³',
    level VARCHAR(20) NOT NULL COMMENT 'æ—¥å¿—çº§åˆ«',
    service VARCHAR(50) NOT NULL COMMENT 'æœåŠ¡åç§°',
    component VARCHAR(100) COMMENT 'ç»„ä»¶åç§°',
    module VARCHAR(200) COMMENT 'æ¨¡å—åç§°',
    message TEXT NOT NULL COMMENT 'æ—¥å¿—æ¶ˆæ¯',
    execution_id INT COMMENT 'æ‰§è¡ŒID',
    trace_id VARCHAR(100) COMMENT 'é“¾è·¯è¿½è¸ªID',
    extra_data JSON COMMENT 'é¢å¤–æ•°æ®',
    created_at DATETIME(6) DEFAULT CURRENT_TIMESTAMP(6),
    updated_at DATETIME(6) DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
    INDEX idx_timestamp (timestamp),
    INDEX idx_service_level (service, level),
    INDEX idx_execution_id (execution_id),
    INDEX idx_trace_id (trace_id),
    FULLTEXT KEY ft_message (message)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='ç»Ÿä¸€æ—¥å¿—å­˜å‚¨è¡¨';

-- åˆ›å»ºæ—¥å¿—ç»Ÿè®¡è¡¨
CREATE TABLE IF NOT EXISTS log_statistics (
    id INT AUTO_INCREMENT PRIMARY KEY,
    service VARCHAR(50) NOT NULL,
    level VARCHAR(20) NOT NULL,
    date_hour DATETIME NOT NULL COMMENT 'æŒ‰å°æ—¶ç»Ÿè®¡',
    count INT DEFAULT 0 COMMENT 'æ—¥å¿—æ•°é‡',
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    UNIQUE KEY uk_service_level_hour (service, level, date_hour),
    INDEX idx_date_hour (date_hour)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æ—¥å¿—ç»Ÿè®¡è¡¨';

-- åˆ›å»ºæ—¥å¿—é…ç½®è¡¨
CREATE TABLE IF NOT EXISTS log_configurations (
    id INT AUTO_INCREMENT PRIMARY KEY,
    service VARCHAR(50) NOT NULL,
    log_level VARCHAR(20) NOT NULL DEFAULT 'INFO',
    enable_realtime BOOLEAN DEFAULT TRUE,
    enable_indexing BOOLEAN DEFAULT TRUE,
    retention_days INT DEFAULT 30,
    config_json JSON COMMENT 'æ‰©å±•é…ç½®',
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    UNIQUE KEY uk_service (service)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æ—¥å¿—é…ç½®è¡¨';

-- æ’å…¥é»˜è®¤é…ç½®
INSERT IGNORE INTO log_configurations (service, log_level, enable_realtime, enable_indexing) VALUES
('django', 'INFO', TRUE, TRUE),
('fastapi', 'INFO', TRUE, TRUE),
('celery', 'INFO', TRUE, TRUE);
'''
        
        schema_file = self.project_root / 'scripts/unified_logs_schema.sql'
        with open(schema_file, 'w', encoding='utf-8') as f:
            f.write(schema_sql)
        
        print(f"âœ… æ•°æ®åº“æ¶æ„æ–‡ä»¶å·²åˆ›å»º: {schema_file}")
        print("   è¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºæ•°æ®åº“è¡¨:")
        print(f"   mysql -u root -p ansflow < {schema_file}")
    
    def generate_consistency_report(self):
        """ç”Ÿæˆæ•°æ®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š"""
        report_content = '''# AnsFlow ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿæ•°æ®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š

## ğŸ“Š å½“å‰æ¶æ„æ¦‚è¿°

### æ•°æ®æŠ“å–å¯¹è±¡åˆ†æ

#### 1. å®æ—¶æ—¥å¿—æŠ“å–å¯¹è±¡ âœ…
- **ä¸»è¦æ•°æ®æº**: Redis Stream (`ansflow:logs:stream` DB 5)  
- **è¾…åŠ©æ•°æ®æº**: æ–‡ä»¶ç³»ç»Ÿç›‘æ§ (`/logs/services/*/`)
- **ä¼ è¾“æ–¹å¼**: WebSocketå®æ—¶æ¨é€
- **æ•°æ®æ ¼å¼**: æ ‡å‡†åŒ–JSONæ ¼å¼
- **å»¶è¿Ÿ**: < 100ms

#### 2. å†å²æŸ¥è¯¢æŠ“å–å¯¹è±¡ âš ï¸
- **ä¸»è¦æ•°æ®æº**: MySQLæ•°æ®åº“ (PipelineExecution, StepExecution)
- **è¾…åŠ©æ•°æ®æº**: æœ¬åœ°æ—¥å¿—æ–‡ä»¶
- **æŸ¥è¯¢æ–¹å¼**: REST API + SQLæŸ¥è¯¢  
- **æ•°æ®æ ¼å¼**: å…³ç³»å‹æ•°æ® + æ–‡æœ¬æ—¥å¿—
- **ä¸€è‡´æ€§**: å­˜åœ¨æ•°æ®åŒæ­¥å»¶è¿Ÿ

#### 3. æ—¥å¿—ç´¢å¼•æŠ“å–å¯¹è±¡ ğŸ”„
- **å½“å‰çŠ¶æ€**: åŸºç¡€æ–‡ä»¶ç´¢å¼• + Redisæ—¶é—´æˆ³ç´¢å¼•
- **è®¡åˆ’ä¸­**: ElasticSearchå…¨æ–‡ç´¢å¼•
- **ç´¢å¼•æ–¹å¼**: æ—¶é—´æˆ³ + æ‰§è¡ŒID + æœåŠ¡å
- **æœç´¢èƒ½åŠ›**: åŸºç¡€å…³é”®è¯åŒ¹é…

## âš ï¸ æ•°æ®ä¸€è‡´æ€§é—®é¢˜

### å‘ç°çš„é—®é¢˜

1. **æ•°æ®æºåˆ†æ•£**
   - å®æ—¶æ—¥å¿— â†’ Redis Stream
   - å†å²æŸ¥è¯¢ â†’ MySQL + æ–‡ä»¶ç³»ç»Ÿ  
   - ç´¢å¼•ç³»ç»Ÿ â†’ å¤šå­˜å‚¨åç«¯
   - ğŸ”´ é£é™©: æ•°æ®ä¸åŒæ­¥ï¼ŒæŸ¥è¯¢ç»“æœä¸ä¸€è‡´

2. **æ ¼å¼ä¸ç»Ÿä¸€**
   - Redis: æ ‡å‡†åŒ–JSONç»“æ„
   - MySQL: å…³ç³»å‹æ•°æ®ç»“æ„
   - æ–‡ä»¶: æ··åˆæ ¼å¼ (JSON + çº¯æ–‡æœ¬)
   - ğŸ”´ é£é™©: è§£æé€»è¾‘å¤æ‚ï¼Œå®¹æ˜“å‡ºé”™

3. **æ—¶æ•ˆæ€§å·®å¼‚**
   - å®æ—¶æ—¥å¿—: æ¯«ç§’çº§å»¶è¿Ÿ
   - å†å²å­˜å‚¨: å¯èƒ½æœ‰ç§’çº§æˆ–åˆ†é’Ÿçº§å»¶è¿Ÿ
   - ç´¢å¼•æ›´æ–°: å¼‚æ­¥å¤„ç†ï¼Œå»¶è¿Ÿä¸ç¡®å®š
   - ğŸ”´ é£é™©: ç”¨æˆ·çœ‹åˆ°çš„æ•°æ®ä¸ä¸€è‡´

### å…·ä½“è¡¨ç°

```bash
# å®æ—¶æ—¥å¿—æ˜¾ç¤º
WebSocketæ—¥å¿—: 4æ¡è®°å½• (1 Django + 3 FastAPI)

# å†å²æŸ¥è¯¢å¯èƒ½æ˜¾ç¤º
MySQLæŸ¥è¯¢: å¯èƒ½æ˜¯0-4æ¡è®°å½• (å–å†³äºåŒæ­¥çŠ¶æ€)

# ç´¢å¼•æœç´¢å¯èƒ½æ˜¾ç¤º  
ElasticSearch: å½“å‰æœªå¯ç”¨
æ–‡ä»¶ç´¢å¼•: åŸºäºæ–‡ä»¶ä¿®æ”¹æ—¶é—´
```

## âœ… è§£å†³æ–¹æ¡ˆ

### 1. ç»Ÿä¸€è¿æ¥å™¨ (å·²å®ç°)
- ğŸ“ `common/unified_log_connector.py`
- ğŸ”§ ç»Ÿä¸€æ‰€æœ‰å­˜å‚¨åç«¯çš„è®¿é—®æ¥å£
- ğŸ¯ ç¡®ä¿å†™å…¥æ“ä½œçš„åŸå­æ€§
- ğŸ“Š æä¾›æ•°æ®ä¸€è‡´æ€§éªŒè¯

### 2. æ•°æ®åŒæ­¥ç­–ç•¥
```python
async def sync_log_entry(self, log_entry: LogEntry):
    # åŒæ­¥å†™å…¥æ‰€æœ‰å­˜å‚¨ç³»ç»Ÿ
    tasks = [
        self.write_realtime_log(log_entry),      # Redis Stream
        self.store_historical_log(log_entry),    # MySQL
        self.index_log(log_entry)                # ElasticSearch
    ]
    await asyncio.gather(*tasks, return_exceptions=True)
```

### 3. ä¸€è‡´æ€§éªŒè¯
```python
async def verify_data_consistency(self, log_id: str) -> Dict[str, bool]:
    # æ£€æŸ¥æ‰€æœ‰å­˜å‚¨ç³»ç»Ÿä¸­çš„æ•°æ®
    return {
        'redis_stream': bool,      # å®æ—¶æµ
        'mysql_historical': bool,  # å†å²æ•°æ®  
        'elasticsearch_index': bool # æœç´¢ç´¢å¼•
    }
```

## ğŸ¯ æ¨èçš„ç»Ÿä¸€æ¶æ„

### æ¶æ„åŸåˆ™
1. **å•ä¸€å†™å…¥ç‚¹**: æ‰€æœ‰æ—¥å¿—é€šè¿‡ç»Ÿä¸€è¿æ¥å™¨å†™å…¥
2. **å¤šå­˜å‚¨åç«¯**: Redis(å®æ—¶) + MySQL(å†å²) + ES(ç´¢å¼•)
3. **ä¸€è‡´æ€§ä¿è¯**: åŒæ­¥å†™å…¥ + å®šæœŸæ ¡éªŒ
4. **ä¼˜é›…é™çº§**: å•ä¸ªå­˜å‚¨å¤±è´¥ä¸å½±å“å…¶ä»–å­˜å‚¨

### æ•°æ®æµå‘
```
æ—¥å¿—äº§ç”Ÿ â†’ ç»Ÿä¸€è¿æ¥å™¨ â†’ {Redis Stream, MySQL, ElasticSearch}
                        â†“
                    å‰ç«¯æŸ¥è¯¢æ¥å£ â† {å®æ—¶æ—¥å¿—, å†å²æŸ¥è¯¢, å…¨æ–‡æœç´¢}
```

### æŸ¥è¯¢ä¼˜å…ˆçº§
1. **å®æ—¶ç›‘æ§**: Redis Stream (æœ€é«˜ä¼˜å…ˆçº§)
2. **å†å²æŸ¥è¯¢**: MySQLæ•°æ®åº“ (ç»“æ„åŒ–æŸ¥è¯¢)
3. **å…¨æ–‡æœç´¢**: ElasticSearch (å¤æ‚æœç´¢)
4. **æ–‡ä»¶ç³»ç»Ÿ**: é™çº§æ–¹æ¡ˆ (ç³»ç»Ÿæ•…éšœæ—¶)

## ğŸ“‹ å®æ–½å»ºè®®

### ç«‹å³å®æ–½ (ä¼˜å…ˆçº§: é«˜)
- [ ] éƒ¨ç½²ç»Ÿä¸€è¿æ¥å™¨åˆ°ç”Ÿäº§ç¯å¢ƒ
- [ ] åˆ›å»ºunified_logsè¡¨ç»“æ„
- [ ] ä¿®æ”¹ç°æœ‰æ—¥å¿—å†™å…¥ä»£ç ä½¿ç”¨ç»Ÿä¸€æ¥å£

### ä¸­æœŸå®æ–½ (ä¼˜å…ˆçº§: ä¸­)
- [ ] é›†æˆElasticSearchç´¢å¼•ç³»ç»Ÿ
- [ ] å®ç°æ•°æ®ä¸€è‡´æ€§ç›‘æ§é¢æ¿
- [ ] æ·»åŠ è‡ªåŠ¨æ•°æ®ä¿®å¤æœºåˆ¶

### é•¿æœŸä¼˜åŒ– (ä¼˜å…ˆçº§: ä½)  
- [ ] å®ç°æ—¥å¿—æ•°æ®å‹ç¼©å’Œå½’æ¡£
- [ ] æ·»åŠ æ—¥å¿—æ•°æ®åˆ†æå’Œå‘Šè­¦
- [ ] æ”¯æŒå¤šç§Ÿæˆ·æ—¥å¿—éš”ç¦»

## ğŸ” éªŒè¯æ–¹æ³•

### è¿æ¥æµ‹è¯•
```bash
# è¿è¡Œè¿æ¥æµ‹è¯•è„šæœ¬
uv run scripts/test_log_connections.py
```

### ä¸€è‡´æ€§éªŒè¯
```bash  
# å†™å…¥æµ‹è¯•æ—¥å¿—
# éªŒè¯ä¸‰ä¸ªå­˜å‚¨ç³»ç»Ÿéƒ½åŒ…å«ç›¸åŒæ•°æ®
# æ£€æŸ¥æŸ¥è¯¢ç»“æœä¸€è‡´æ€§
```

### æ€§èƒ½æµ‹è¯•
```bash
# æµ‹è¯•å¹¶å‘å†™å…¥æ€§èƒ½
# æµ‹è¯•æŸ¥è¯¢å“åº”æ—¶é—´
# æµ‹è¯•æ•…éšœæ¢å¤èƒ½åŠ›
```

---

**æ€»ç»“**: å½“å‰ç³»ç»Ÿå­˜åœ¨æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼Œä½†å·²æœ‰å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å®æ–½ç»Ÿä¸€è¿æ¥å™¨å’Œæ ‡å‡†åŒ–æ•°æ®æ ¼å¼ï¼Œå¯ä»¥ç¡®ä¿å®æ—¶æ—¥å¿—ã€å†å²æŸ¥è¯¢å’Œæ—¥å¿—ç´¢å¼•çš„æ•°æ®å®Œå…¨ä¸€è‡´ã€‚
'''
        
        report_file = self.project_root / 'docs/æ—¥å¿—ç³»ç»Ÿæ•°æ®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… æ•°æ®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AnsFlowç»Ÿä¸€æ—¥å¿—ç³»ç»ŸUVè¿æ¥ç®¡ç†å™¨')
    parser.add_argument('action', choices=[
        'install', 'setup', 'test', 'schema', 'report', 'all'
    ], help='è¦æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--service', default='all', 
                       choices=['all', 'django', 'fastapi'],
                       help='ç›®æ ‡æœåŠ¡')
    parser.add_argument('--root', default='/Users/creed/Workspace/OpenSource/ansflow',
                       help='é¡¹ç›®æ ¹ç›®å½•')
    
    args = parser.parse_args()
    
    connector = UVLogConnector(args.root)
    
    if args.action == 'install' or args.action == 'all':
        print("ğŸ“¦ å®‰è£…æ—¥å¿—ç³»ç»Ÿä¾èµ–...")
        connector.install_dependencies(args.service)
    
    if args.action == 'setup' or args.action == 'all':
        print("ğŸ”§ è®¾ç½®ç»Ÿä¸€æ—¥å¿—é…ç½®...")
        connector.setup_unified_logging()
    
    if args.action == 'test' or args.action == 'all':
        print("ğŸ§ª åˆ›å»ºè¿æ¥æµ‹è¯•è„šæœ¬...")
        connector.create_connection_script()
    
    if args.action == 'schema' or args.action == 'all':
        print("ğŸ—„ï¸  åˆ›å»ºæ•°æ®åº“æ¶æ„...")
        connector.create_database_schema()
    
    if args.action == 'report' or args.action == 'all':
        print("ğŸ“Š ç”Ÿæˆä¸€è‡´æ€§åˆ†ææŠ¥å‘Š...")
        connector.generate_consistency_report()
    
    if args.action == 'all':
        print("\nğŸ‰ AnsFlowç»Ÿä¸€æ—¥å¿—ç³»ç»ŸUVè¿æ¥ç®¡ç†å®Œæˆï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œï¼š")
        print("1. è¿è¡Œæ•°æ®åº“æ¶æ„ï¼šmysql -u root -p ansflow < scripts/unified_logs_schema.sql")
        print("2. æµ‹è¯•è¿æ¥çŠ¶æ€ï¼šuv run scripts/test_log_connections.py")
        print("3. æŸ¥çœ‹åˆ†ææŠ¥å‘Šï¼šdocs/æ—¥å¿—ç³»ç»Ÿæ•°æ®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š.md")


if __name__ == "__main__":
    main()

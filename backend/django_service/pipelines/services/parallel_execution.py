"""
å¹¶è¡Œæ‰§è¡ŒæœåŠ¡
è´Ÿè´£å¤„ç†æµæ°´çº¿ä¸­çš„å¹¶è¡Œç»„æ‰§è¡Œé€»è¾‘ï¼Œæ”¯æŒæœ¬åœ°å’Œè¿œç¨‹å¹¶è¡Œæ‰§è¡Œ
"""
import logging
import asyncio
import time
import json
import statistics
import hashlib
from common.execution_logger import ExecutionLogger
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple
from django.utils import timezone
from django.db import transaction
from ..models import Pipeline, PipelineRun, ParallelGroup
from cicd_integrations.models import AtomicStep, StepExecution, PipelineExecution
from pipelines.services.local_executor import LocalPipelineExecutor
# from cicd_integrations.executors.remote_executor import RemoteStepExecutor  # æš‚æ—¶ç¦ç”¨

# å¯é€‰ä¾èµ–
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    psutil = None

logger = logging.getLogger(__name__)


class ParallelExecutionService:
    """å¹¶è¡Œæ‰§è¡ŒæœåŠ¡"""
    
    def __init__(self):
        self.local_executor = LocalPipelineExecutor()
        # self.remote_executor = RemoteStepExecutor()  # æš‚æ—¶ç¦ç”¨
        self.remote_executor = None
        self.max_parallel_workers = 10  # æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
    
    def analyze_pipeline_execution_plan(self, pipeline: Pipeline) -> Dict[str, Any]:
        """
        åˆ†ææµæ°´çº¿çš„æ‰§è¡Œè®¡åˆ’ï¼Œè¯†åˆ«å¹¶è¡Œç»„å’Œä¾èµ–å…³ç³»
        
        Returns:
            æ‰§è¡Œè®¡åˆ’ï¼ŒåŒ…å«æ­¥éª¤åˆ†ç»„ã€ä¾èµ–å…³ç³»å’Œæ‰§è¡Œé¡ºåº
        """
        steps = list(pipeline.atomic_steps.all().order_by('order'))
        parallel_groups = {}
        sequential_steps = []
        execution_plan = {
            'stages': [],  # æ‰§è¡Œé˜¶æ®µåˆ—è¡¨
            'parallel_groups': {},
            'dependencies': {},
            'total_stages': 0
        }
        
        # æ”¶é›†å¹¶è¡Œç»„ä¿¡æ¯
        for step in steps:
            if hasattr(step, 'parallel_group') and step.parallel_group:
                group_id = step.parallel_group
                if group_id not in parallel_groups:
                    parallel_groups[group_id] = {
                        'id': group_id,
                        'name': f"å¹¶è¡Œç»„-{group_id[:8]}",
                        'steps': [],
                        'sync_policy': 'wait_all',  # é»˜è®¤ç­‰å¾…æ‰€æœ‰æ­¥éª¤å®Œæˆ
                        'timeout_seconds': None,
                        'min_order': float('inf'),
                        'max_order': 0
                    }
                
                parallel_groups[group_id]['steps'].append(step)
                parallel_groups[group_id]['min_order'] = min(parallel_groups[group_id]['min_order'], step.order)
                parallel_groups[group_id]['max_order'] = max(parallel_groups[group_id]['max_order'], step.order)
                logger.info(f"Added step {step.name} to parallel group {group_id} with order {step.order}")
            else:
                sequential_steps.append(step)
                logger.info(f"Added step {step.name} to sequential execution with order {step.order}")
        
        logger.info(f"Found {len(parallel_groups)} parallel groups and {len(sequential_steps)} sequential steps")
        
        # åˆ›å»ºæ‰§è¡Œé˜¶æ®µ
        # æŒ‰ç…§ order å­—æ®µæ’åºï¼Œå°†å¹¶è¡Œç»„å’Œå•ç‹¬æ­¥éª¤å®‰æ’åˆ°ä¸åŒçš„é˜¶æ®µ
        all_items = []
        
        # æ·»åŠ å•ç‹¬æ­¥éª¤
        for step in sequential_steps:
            all_items.append({
                'type': 'step',
                'order': step.order,
                'item': step,
                'parallel': False
            })
        
        # æ·»åŠ å¹¶è¡Œç»„ï¼ˆä½¿ç”¨æœ€å°orderä½œä¸ºç»„çš„orderï¼‰
        for group_id, group_info in parallel_groups.items():
            all_items.append({
                'type': 'parallel_group',
                'order': group_info['min_order'],
                'item': group_info,
                'parallel': True
            })
        
        # æŒ‰orderæ’åº
        all_items.sort(key=lambda x: x['order'])
        
        # æ„å»ºæ‰§è¡Œé˜¶æ®µ
        stage_number = 0
        for item in all_items:
            stage = {
                'stage_number': stage_number,
                'type': item['type'],
                'parallel': item['parallel'],
                'items': []
            }
            
            if item['type'] == 'step':
                stage['items'] = [item['item']]
            else:  # parallel_group
                stage['items'] = item['item']['steps']
                stage['group_info'] = item['item']
            
            execution_plan['stages'].append(stage)
            stage_number += 1
        
        execution_plan['total_stages'] = stage_number
        execution_plan['parallel_groups'] = parallel_groups
        
        logger.info(f"Pipeline {pipeline.id} execution plan: {stage_number} stages, {len(parallel_groups)} parallel groups")
        return execution_plan
    
    def execute_pipeline_with_parallel_support(self, 
                                              pipeline: Pipeline, 
                                              pipeline_run: PipelineRun, 
                                              execution_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®æ‰§è¡Œè®¡åˆ’æ‰§è¡Œæµæ°´çº¿ï¼Œæ”¯æŒå¹¶è¡Œç»„
        """
        logger.info(f"Starting pipeline {pipeline.id} execution with parallel support")
        
        try:
            # åˆ›å»ºæµæ°´çº¿æ‰§è¡Œè®°å½•
            pipeline_execution = self._create_pipeline_execution(pipeline, pipeline_run)
            
            # ğŸ”¥ ä¿®å¤ï¼šåœ¨æœ€å¼€å§‹åˆ›å»ºä¸€æ¬¡å·¥ä½œç©ºé—´ï¼Œæ•´ä¸ªæµæ°´çº¿å…±äº«
            from cicd_integrations.executors.execution_context import ExecutionContext
            
            execution_context = ExecutionContext(
                execution_id=pipeline_execution.id,
                pipeline_name=pipeline_execution.pipeline.name,
                trigger_type='manual'
            )
            
            # å…±äº«çš„å·¥ä½œç›®å½•çŠ¶æ€ - å¯ä»¥è¢«æ­¥éª¤æ›´æ–°
            shared_workspace_state = {
                'working_directory': execution_context.get_workspace_path(),
                'execution_id': pipeline_execution.id,
                'pipeline_name': pipeline_execution.pipeline.name
            }
            
            logger.info(f"ğŸ  åˆ›å»ºå…±äº«å·¥ä½œç©ºé—´: {shared_workspace_state['working_directory']}")
            
            # æŒ‰é˜¶æ®µæ‰§è¡Œï¼Œä¼ é€’å…±äº«çš„å·¥ä½œç©ºé—´çŠ¶æ€
            for stage in execution_plan['stages']:
                stage_result = self._execute_stage(
                    stage, 
                    pipeline, 
                    pipeline_execution, 
                    execution_plan,
                    shared_workspace_state  # ä¼ é€’å…±äº«çŠ¶æ€
                )
                
                if not stage_result['success']:
                    logger.error(f"Stage {stage['stage_number']} failed: {stage_result['message']}")
                    return {
                        'success': False,
                        'message': f"Pipeline failed at stage {stage['stage_number']}: {stage_result['message']}",
                        'failed_stage': stage['stage_number']
                    }
                
                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœé˜¶æ®µæ›´æ–°äº†å·¥ä½œç›®å½•ï¼Œæ›´æ–°å…±äº«çŠ¶æ€
                if stage_result.get('updated_workspace_state'):
                    shared_workspace_state.update(stage_result['updated_workspace_state'])
                    logger.info(f"ğŸ”„ æ›´æ–°å…±äº«å·¥ä½œç›®å½•: {shared_workspace_state['working_directory']}")
            
            # æ‰€æœ‰é˜¶æ®µå®Œæˆ
            ExecutionLogger.complete_execution(
                pipeline_execution,
                status='success',
                log_message='Pipeline completed successfully'
            )
            
            return {
                'success': True,
                'message': 'Pipeline completed successfully',
                'execution_id': pipeline_execution.id
            }
            
        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}")
            if 'pipeline_execution' in locals():
                ExecutionLogger.fail_execution(
                    pipeline_execution,
                    error_message=f'Pipeline execution failed: {str(e)}',
                    log_message=f"Pipeline execution failed: {e}"
                )
            
            return {
                'success': False,
                'message': f'Pipeline execution failed: {str(e)}'
            }
    
    def _execute_stage(self, 
                      stage: Dict[str, Any], 
                      pipeline: Pipeline, 
                      pipeline_execution,
                      execution_plan: Dict[str, Any],
                      shared_workspace_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªé˜¶æ®µï¼ˆå¯èƒ½åŒ…å«å¹¶è¡Œæ­¥éª¤ï¼‰
        """
        stage_number = stage['stage_number']
        is_parallel = stage['parallel']
        steps = stage['items']
        
        logger.info(f"å¼€å§‹æ‰§è¡Œé˜¶æ®µ {stage_number}: {'å¹¶è¡Œ' if is_parallel else 'ä¸²è¡Œ'} æ‰§è¡Œï¼ŒåŒ…å« {len(steps)} ä¸ªæ­¥éª¤")
        
        if is_parallel:
            group_info = stage.get('group_info', {})
            logger.info(f"  - å¹¶è¡Œç»„ID: {group_info.get('id', 'N/A')}")
            logger.info(f"  - åŒæ­¥ç­–ç•¥: {group_info.get('sync_policy', 'wait_all')}")
            return self._execute_parallel_stage(stage, pipeline, pipeline_execution, shared_workspace_state)
        else:
            return self._execute_sequential_stage(stage, pipeline, pipeline_execution, shared_workspace_state)
    
    def _execute_parallel_stage(self, 
                               stage: Dict[str, Any], 
                               pipeline: Pipeline, 
                               pipeline_execution) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¹¶è¡Œé˜¶æ®µ
        """
        steps = stage['items']
        group_info = stage.get('group_info', {})
        sync_policy = group_info.get('sync_policy', 'wait_all')
        timeout_seconds = group_info.get('timeout_seconds')
        
        logger.info(f"Executing parallel stage with {len(steps)} steps, sync_policy: {sync_policy}")
        
        # æ ¹æ®æ‰§è¡Œæ¨¡å¼é€‰æ‹©å¹¶è¡Œæ‰§è¡Œç­–ç•¥
        if pipeline.execution_mode == 'local':
            return self._execute_parallel_local(steps, pipeline_execution, sync_policy, timeout_seconds)
        elif pipeline.execution_mode == 'remote':
            return self._execute_parallel_remote(steps, pipeline, pipeline_execution, sync_policy)
        else:  # hybrid
            return self._execute_parallel_hybrid(steps, pipeline, pipeline_execution, sync_policy)
    
    def _execute_parallel_local(self, 
                               steps: List[AtomicStep], 
                               pipeline_execution,
                               sync_policy: str = 'wait_all',
                               timeout_seconds: Optional[int] = None) -> Dict[str, Any]:
        """
        æœ¬åœ°å¹¶è¡Œæ‰§è¡Œï¼ˆä½¿ç”¨çº¿ç¨‹æ± ï¼‰
        """
        logger.info(f"å¼€å§‹æœ¬åœ°å¹¶è¡Œæ‰§è¡Œ {len(steps)} ä¸ªæ­¥éª¤ï¼ŒåŒæ­¥ç­–ç•¥: {sync_policy}")
        
        # æ‰“å°æ­¥éª¤ä¿¡æ¯
        for step in steps:
            logger.info(f"  - æ­¥éª¤: {step.name}, é¡ºåº: {step.order}, ç±»å‹: {step.step_type}")
        
        step_executions = []
        for step in steps:
            step_execution = StepExecution.objects.create(
                pipeline_execution=pipeline_execution,
                atomic_step=step,
                status='pending',
                order=step.order
            )
            step_executions.append(step_execution)
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶è¡Œæ­¥éª¤
        results = []
        logger.info(f"åˆ›å»ºçº¿ç¨‹æ± ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {min(len(steps), self.max_parallel_workers)}")
        
        with ThreadPoolExecutor(max_workers=min(len(steps), self.max_parallel_workers)) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_step = {
                executor.submit(self._execute_step_local, step_execution): step_execution
                for step_execution in step_executions
            }
            
            logger.info(f"å·²æäº¤ {len(future_to_step)} ä¸ªå¹¶è¡Œä»»åŠ¡åˆ°çº¿ç¨‹æ± ")
            
            # ç­‰å¾…ç»“æœ
            completed_count = 0
            failed_count = 0
            
            try:
                for future in as_completed(future_to_step, timeout=timeout_seconds):
                    step_execution = future_to_step[future]
                    try:
                        result = future.result()
                        results.append(result)
                        
                        if result['success']:
                            completed_count += 1
                            step_execution.status = 'success'
                            logger.info(f"æ­¥éª¤ {step_execution.atomic_step.name} æ‰§è¡ŒæˆåŠŸ")
                        else:
                            failed_count += 1
                            step_execution.status = 'failed'
                            step_execution.error_message = result.get('error', 'Unknown error')
                            logger.error(f"æ­¥éª¤ {step_execution.atomic_step.name} æ‰§è¡Œå¤±è´¥: {result.get('error', 'Unknown error')}")
                        
                        step_execution.completed_at = timezone.now()
                        step_execution.save()
                        
                        # æ ¹æ®åŒæ­¥ç­–ç•¥å†³å®šæ˜¯å¦æå‰é€€å‡º
                        if sync_policy == 'fail_fast' and failed_count > 0:
                            logger.info("Fail-fastç­–ç•¥è§¦å‘ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡")
                            # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„æ­¥éª¤
                            self._cancel_parallel_remaining_steps(
                                step_executions, completed_count + failed_count, 
                                f"å¹¶è¡Œç»„ä¸­æœ‰æ­¥éª¤å¤±è´¥ï¼Œç­–ç•¥ä¸ºfail_fast"
                            )
                            break
                        elif sync_policy == 'wait_any' and completed_count > 0:
                            logger.info("Wait-anyç­–ç•¥æ»¡è¶³ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡")
                            # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„æ­¥éª¤
                            self._cancel_parallel_remaining_steps(
                                step_executions, completed_count + failed_count,
                                f"å¹¶è¡Œç»„ä¸­å·²æœ‰æ­¥éª¤å®Œæˆï¼Œç­–ç•¥ä¸ºwait_any"
                            )
                            break
                            
                    except Exception as e:
                        logger.error(f"æ­¥éª¤æ‰§è¡Œå¼‚å¸¸: {e}")
                        step_execution.status = 'failed'
                        step_execution.error_message = str(e)
                        step_execution.completed_at = timezone.now()
                        step_execution.save()
                        failed_count += 1
            
            except asyncio.TimeoutError:
                logger.error(f"å¹¶è¡Œæ‰§è¡Œè¶…æ—¶ï¼Œè¶…æ—¶æ—¶é—´: {timeout_seconds} ç§’")
                return {
                    'success': False,
                    'message': f'å¹¶è¡Œæ‰§è¡Œè¶…æ—¶ï¼Œè¶…æ—¶æ—¶é—´: {timeout_seconds} ç§’'
                }
        
        # è¯„ä¼°æ•´ä½“ç»“æœ
        total_steps = len(steps)
        success = self._evaluate_parallel_result(sync_policy, completed_count, failed_count, total_steps)
        
        logger.info(f"å¹¶è¡Œæ‰§è¡Œå®Œæˆ: {completed_count} æˆåŠŸ, {failed_count} å¤±è´¥, æ€»ä½“ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        return {
            'success': success,
            'message': f'å¹¶è¡Œæ‰§è¡Œå®Œæˆ: {completed_count} æˆåŠŸ, {failed_count} å¤±è´¥',
            'completed_count': completed_count,
            'failed_count': failed_count,
            'sync_policy': sync_policy
        }
    
    def _execute_parallel_remote(self, 
                                steps: List[AtomicStep], 
                                pipeline: Pipeline,
                                pipeline_execution,
                                sync_policy: str = 'wait_all') -> Dict[str, Any]:
        """
        è¿œç¨‹å¹¶è¡Œæ‰§è¡Œï¼ˆè½¬æ¢ä¸ºç›®æ ‡CI/CDå·¥å…·çš„å¹¶è¡Œè¯­æ³•ï¼‰
        """
        tool = pipeline.execution_tool
        if not tool:
            return {
                'success': False,
                'message': 'No execution tool configured for remote parallel execution'
            }
        
        if tool.tool_type == 'jenkins':
            return self._execute_parallel_jenkins(steps, pipeline, pipeline_execution, sync_policy)
        elif tool.tool_type == 'gitlab_ci':
            return self._execute_parallel_gitlab(steps, pipeline, pipeline_execution, sync_policy)
        elif tool.tool_type == 'github_actions':
            return self._execute_parallel_github(steps, pipeline, pipeline_execution, sync_policy)
        else:
            return {
                'success': False,
                'message': f'Parallel execution not supported for tool type: {tool.tool_type}'
            }
    
    def _execute_parallel_jenkins(self, 
                                 steps: List[AtomicStep], 
                                 pipeline: Pipeline,
                                 pipeline_execution,
                                 sync_policy: str) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºJenkinså¹¶è¡Œæ‰§è¡Œè¯­æ³•
        """
        logger.info(f"Converting {len(steps)} steps to Jenkins parallel syntax")
        
        # ç”ŸæˆJenkins Pipelineå¹¶è¡Œä»£ç 
        parallel_code = self._generate_jenkins_parallel_code(steps, sync_policy)
        
        # æ›´æ–°Jenkinsä½œä¸šé…ç½®
        tool = pipeline.execution_tool
        jenkins_service = self._get_jenkins_service(tool)
        
        try:
            # æ‰§è¡ŒJenkinsä½œä¸š
            build_result = jenkins_service.trigger_parallel_build(
                job_name=pipeline.tool_job_name,
                parallel_code=parallel_code,
                parameters=pipeline_execution.parameters
            )
            
            # åˆ›å»ºæ­¥éª¤æ‰§è¡Œè®°å½•
            for step in steps:
                StepExecution.objects.create(
                    pipeline_execution=pipeline_execution,
                    atomic_step=step,
                    external_id=build_result.get('build_number'),
                    status='running',
                    order=step.order
                )
            
            return {
                'success': True,
                'message': 'Jenkins parallel execution started',
                'build_number': build_result.get('build_number'),
                'jenkins_url': build_result.get('url')
            }
            
        except Exception as e:
            logger.error(f"Jenkins parallel execution failed: {e}")
            return {
                'success': False,
                'message': f'Jenkins parallel execution failed: {str(e)}'
            }
    
    def _generate_jenkins_parallel_code(self, steps: List[AtomicStep], sync_policy: str) -> str:
        """
        ç”ŸæˆJenkins Pipelineçš„å¹¶è¡Œæ‰§è¡Œä»£ç 
        """
        parallel_blocks = []
        
        for step in steps:
            step_name = step.name.replace(' ', '_').replace('-', '_')
            step_code = f'''
            "{step_name}": {{
                node {{
                    stage("{step.name}") {{
                        try {{
                            {self._convert_step_to_jenkins_code(step)}
                        }} catch (Exception e) {{
                            currentBuild.result = 'FAILURE'
                            throw e
                        }}
                    }}
                }}
            }}'''
            parallel_blocks.append(step_code)
        
        # æ ¹æ®åŒæ­¥ç­–ç•¥æ·»åŠ å¤±è´¥å¤„ç†
        fail_fast = "true" if sync_policy == "fail_fast" else "false"
        
        jenkins_code = f'''
        pipeline {{
            agent any
            stages {{
                stage('Parallel Execution') {{
                    steps {{
                        script {{
                            parallel(
                                failFast: {fail_fast},
                                {','.join(parallel_blocks)}
                            )
                        }}
                    }}
                }}
            }}
        }}
        '''
        
        return jenkins_code
    
    def _convert_step_to_jenkins_code(self, step: AtomicStep) -> str:
        """
        å°†æ­¥éª¤è½¬æ¢ä¸ºJenkinsæ‰§è¡Œä»£ç 
        """
        if step.step_type == 'shell':
            return f'sh """{step.config.get("command", "")}"""'
        elif step.step_type == 'python':
            script = step.config.get("script", "")
            return f'sh """python3 -c \'{script}\' """'
        elif step.step_type == 'docker':
            image = step.config.get('image', 'ubuntu:latest')
            command = step.config.get('command', 'echo "Hello World"')
            return f'sh """docker run --rm {image} {command}"""'
        else:
            return f'echo "Executing step: {step.name}"'
    
    def _execute_sequential_stage(self, 
                                 stage: Dict[str, Any], 
                                 pipeline: Pipeline, 
                                 pipeline_execution) -> Dict[str, Any]:
        """
        æ‰§è¡Œé¡ºåºé˜¶æ®µ - æ”¯æŒå¤±è´¥ä¸­æ–­åŠŸèƒ½
        """
        steps = stage['items']
        logger.info(f"Executing sequential stage with {len(steps)} steps")
        
        failed_step_index = -1
        failed_step_name = None
        
        for index, step in enumerate(steps):
            step_execution = StepExecution.objects.create(
                pipeline_execution=pipeline_execution,
                atomic_step=step,
                status='running',
                order=step.order
            )
            
            try:
                result = self._execute_step_local(step_execution)
                
                if result['success']:
                    step_execution.status = 'success'
                else:
                    step_execution.status = 'failed'
                    step_execution.error_message = result.get('error', 'Unknown error')
                    step_execution.completed_at = timezone.now()
                    step_execution.save()
                    
                    # è®°å½•å¤±è´¥çš„æ­¥éª¤ä¿¡æ¯
                    failed_step_index = index
                    failed_step_name = step.name
                    
                    # å–æ¶ˆåç»­æ­¥éª¤å¹¶è®¾ç½®çŠ¶æ€
                    self._cancel_remaining_steps(steps[index + 1:], pipeline_execution, step.name)
                    
                    return {
                        'success': False,
                        'message': f'Step {step.name} failed: {result.get("error", "Unknown error")}',
                        'failed_step': step.name,
                        'cancelled_steps': len(steps) - index - 1
                    }
                
                step_execution.completed_at = timezone.now()
                step_execution.save()
                
            except Exception as e:
                logger.error(f"Step execution error: {e}")
                step_execution.status = 'failed'
                step_execution.error_message = str(e)
                step_execution.completed_at = timezone.now()
                step_execution.save()
                
                # è®°å½•å¤±è´¥çš„æ­¥éª¤ä¿¡æ¯
                failed_step_index = index
                failed_step_name = step.name
                
                # å–æ¶ˆåç»­æ­¥éª¤å¹¶è®¾ç½®çŠ¶æ€
                self._cancel_remaining_steps(steps[index + 1:], pipeline_execution, step.name)
                
                return {
                    'success': False,
                    'message': f'Step {step.name} failed: {str(e)}',
                    'failed_step': step.name,
                    'cancelled_steps': len(steps) - index - 1
                }
        
        return {
            'success': True,
            'message': f'Sequential stage completed with {len(steps)} steps'
        }
    
    def _cancel_remaining_steps(self, remaining_steps: List[Any], pipeline_execution, failed_step_name: str):
        """
        å–æ¶ˆå‰©ä½™æ­¥éª¤æ‰§è¡Œï¼Œè®¾ç½®å–æ¶ˆçŠ¶æ€å’Œæç¤ºæ¶ˆæ¯
        
        Args:
            remaining_steps: å‰©ä½™å¾…æ‰§è¡Œçš„æ­¥éª¤åˆ—è¡¨
            pipeline_execution: æµæ°´çº¿æ‰§è¡Œå®ä¾‹
            failed_step_name: å¤±è´¥æ­¥éª¤çš„åç§°
        """
        if not remaining_steps:
            return
            
        logger.info(f"Cancelling {len(remaining_steps)} remaining steps due to failure in step '{failed_step_name}'")
        
        for step in remaining_steps:
            step_execution = StepExecution.objects.create(
                pipeline_execution=pipeline_execution,
                atomic_step=step,
                status='cancelled',
                order=step.order,
                error_message=f"å‰é¢æœ‰å¤±è´¥çš„æ­¥éª¤ï¼ˆ{failed_step_name}ï¼‰ï¼Œåé¢æ­¥éª¤å–æ¶ˆæ‰§è¡Œ",
                completed_at=timezone.now()
            )
            
            logger.info(f"Step '{step.name}' cancelled due to previous failure in '{failed_step_name}'")
    
    def _cancel_parallel_remaining_steps(self, step_executions: List[Any], completed_index: int, reason: str):
        """
        å–æ¶ˆå¹¶è¡Œç»„ä¸­å‰©ä½™çš„æ­¥éª¤æ‰§è¡Œ
        
        Args:
            step_executions: æ­¥éª¤æ‰§è¡Œå®ä¾‹åˆ—è¡¨
            completed_index: å·²å®Œæˆæ­¥éª¤çš„ç´¢å¼•
            reason: å–æ¶ˆåŸå› 
        """
        cancelled_count = 0
        
        for i, step_execution in enumerate(step_executions):
            if i >= completed_index and step_execution.status in ['pending', 'running']:
                step_execution.status = 'cancelled'
                step_execution.error_message = f"å‰é¢æœ‰å¤±è´¥çš„æ­¥éª¤ï¼Œåé¢æ­¥éª¤å–æ¶ˆæ‰§è¡Œ: {reason}"
                step_execution.completed_at = timezone.now()
                step_execution.save()
                cancelled_count += 1
                
                logger.info(f"Parallel step '{step_execution.atomic_step.name}' cancelled: {reason}")
        
        if cancelled_count > 0:
            logger.info(f"Cancelled {cancelled_count} parallel steps due to: {reason}")
    
    def _execute_step_local(self, step_execution) -> Dict[str, Any]:
        """
        æœ¬åœ°æ‰§è¡Œå•ä¸ªæ­¥éª¤
        """
        try:
            step = step_execution.atomic_step
            logger.info(f"å¼€å§‹æœ¬åœ°æ‰§è¡Œæ­¥éª¤: {step.name}")
            
            # æ›´æ–°çŠ¶æ€
            step_execution.status = 'running'
            step_execution.started_at = timezone.now()
            step_execution.save()
            
            # æ‰§è¡Œæ­¥éª¤
            result = self.local_executor.execute_step(step)
            
            # ä¿å­˜è¾“å‡º
            step_execution.logs = result.get('output', '')
            step_execution.output = result.get('result', {})
            
            logger.info(f"æ­¥éª¤ {step.name} æ‰§è¡Œå®Œæˆï¼Œç»“æœ: {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}")
            
            return result
            
        except Exception as e:
            logger.error(f"æœ¬åœ°æ­¥éª¤æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _evaluate_parallel_result(self, sync_policy: str, completed: int, failed: int, total: int) -> bool:
        """
        æ ¹æ®åŒæ­¥ç­–ç•¥è¯„ä¼°å¹¶è¡Œæ‰§è¡Œç»“æœ
        """
        if sync_policy == 'wait_all':
            return failed == 0  # æ‰€æœ‰æ­¥éª¤éƒ½å¿…é¡»æˆåŠŸ
        elif sync_policy == 'wait_any':
            return completed > 0  # è‡³å°‘ä¸€ä¸ªæ­¥éª¤æˆåŠŸ
        elif sync_policy == 'fail_fast':
            return failed == 0  # æ²¡æœ‰å¤±è´¥çš„æ­¥éª¤
        else:
            return completed > failed  # é»˜è®¤ï¼šæˆåŠŸå¤šäºå¤±è´¥
    
    def _create_pipeline_execution(self, pipeline: Pipeline, pipeline_run: PipelineRun):
        """
        åˆ›å»ºæµæ°´çº¿æ‰§è¡Œè®°å½•
        """
        from cicd_integrations.models import PipelineExecution
        
        return PipelineExecution.objects.create(
            pipeline=pipeline,
            status='running',
            trigger_type='manual',
            definition=pipeline.config,
            parameters=pipeline_run.trigger_data,
            started_at=timezone.now()
        )
    
    def _get_jenkins_service(self, tool):
        """
        è·å–JenkinsæœåŠ¡å®ä¾‹
        """
        # è¿™é‡Œåº”è¯¥è¿”å›JenkinsæœåŠ¡å®ä¾‹
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿå¯¹è±¡
        class MockJenkinsService:
            def trigger_parallel_build(self, job_name, parallel_code, parameters):
                return {
                    'build_number': 123,
                    'url': f'http://jenkins.example.com/job/{job_name}/123/'
                }
        
        return MockJenkinsService()
    
    def _execute_parallel_gitlab(self, 
                                steps: List[AtomicStep], 
                                pipeline: Pipeline,
                                pipeline_execution,
                                sync_policy: str) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºGitLab CIå¹¶è¡Œæ‰§è¡Œè¯­æ³•
        """
        logger.info(f"Converting {len(steps)} steps to GitLab CI parallel syntax")
        
        # ç”ŸæˆGitLab CI YAMLé…ç½®
        gitlab_config = self._generate_gitlab_parallel_config(steps, sync_policy)
        
        # æ›´æ–°GitLabé¡¹ç›®é…ç½®
        tool = pipeline.execution_tool
        gitlab_service = self._get_gitlab_service(tool)
        
        try:
            # è§¦å‘GitLab Pipeline
            pipeline_result = gitlab_service.trigger_parallel_pipeline(
                project_id=pipeline.tool_project_id,
                gitlab_config=gitlab_config,
                variables=pipeline_execution.parameters,
                ref=pipeline.tool_branch or 'main'
            )
            
            # åˆ›å»ºæ­¥éª¤æ‰§è¡Œè®°å½•
            for step in steps:
                StepExecution.objects.create(
                    pipeline_execution=pipeline_execution,
                    atomic_step=step,
                    external_id=pipeline_result.get('pipeline_id'),
                    status='running',
                    order=step.order
                )
            
            return {
                'success': True,
                'message': 'GitLab CI parallel execution started',
                'pipeline_id': pipeline_result.get('pipeline_id'),
                'gitlab_url': pipeline_result.get('web_url')
            }
            
        except Exception as e:
            logger.error(f"GitLab CI parallel execution failed: {e}")
            return {
                'success': False,
                'message': f'GitLab CI parallel execution failed: {str(e)}'
            }
    
    def _execute_parallel_github(self, 
                                steps: List[AtomicStep], 
                                pipeline: Pipeline,
                                pipeline_execution,
                                sync_policy: str) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºGitHub Actionså¹¶è¡Œæ‰§è¡Œè¯­æ³•
        """
        logger.info(f"Converting {len(steps)} steps to GitHub Actions parallel syntax")
        
        # ç”ŸæˆGitHub Actions workflowé…ç½®
        workflow_config = self._generate_github_parallel_workflow(steps, sync_policy)
        
        # æ›´æ–°GitHubä»“åº“workflow
        tool = pipeline.execution_tool
        github_service = self._get_github_service(tool)
        
        try:
            # è§¦å‘GitHub Actions workflow
            workflow_result = github_service.trigger_parallel_workflow(
                owner=pipeline.tool_owner,
                repo=pipeline.tool_repo,
                workflow_config=workflow_config,
                inputs=pipeline_execution.parameters,
                ref=pipeline.tool_branch or 'main'
            )
            
            # åˆ›å»ºæ­¥éª¤æ‰§è¡Œè®°å½•
            for step in steps:
                StepExecution.objects.create(
                    pipeline_execution=pipeline_execution,
                    atomic_step=step,
                    external_id=workflow_result.get('run_id'),
                    status='running',
                    order=step.order
                )
            
            return {
                'success': True,
                'message': 'GitHub Actions parallel execution started',
                'run_id': workflow_result.get('run_id'),
                'github_url': workflow_result.get('html_url')
            }
            
        except Exception as e:
            logger.error(f"GitHub Actions parallel execution failed: {e}")
            return {
                'success': False,
                'message': f'GitHub Actions parallel execution failed: {str(e)}'
            }

    def _execute_parallel_hybrid(self, 
                               steps: List[AtomicStep], 
                               pipeline: Pipeline,
                               pipeline_execution,
                               sync_policy: str) -> Dict[str, Any]:
        """
        æ··åˆæ¨¡å¼å¹¶è¡Œæ‰§è¡Œï¼šæ™ºèƒ½åˆ†é…æœ¬åœ°å’Œè¿œç¨‹æ‰§è¡Œ
        """
        logger.info(f"Hybrid parallel execution of {len(steps)} steps")
        
        # åˆ†ææ­¥éª¤ï¼Œå†³å®šæ‰§è¡Œä½ç½®
        local_steps = []
        remote_steps = []
        
        for step in steps:
            if self._should_execute_locally_in_parallel(step):
                local_steps.append(step)
            else:
                remote_steps.append(step)
        
        logger.info(f"Hybrid allocation: {len(local_steps)} local, {len(remote_steps)} remote")
        
        # å¹¶è¡Œæ‰§è¡Œæœ¬åœ°å’Œè¿œç¨‹æ­¥éª¤
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        results = []
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = []
            
            # æäº¤æœ¬åœ°å¹¶è¡Œä»»åŠ¡
            if local_steps:
                local_future = executor.submit(
                    self._execute_parallel_local,
                    local_steps, pipeline_execution, sync_policy
                )
                futures.append(('local', local_future))
            
            # æäº¤è¿œç¨‹å¹¶è¡Œä»»åŠ¡
            if remote_steps:
                remote_future = executor.submit(
                    self._execute_parallel_remote,
                    remote_steps, pipeline, pipeline_execution, sync_policy
                )
                futures.append(('remote', remote_future))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for task_type, future in futures:
                try:
                    result = future.result()
                    result['task_type'] = task_type
                    results.append(result)
                except Exception as e:
                    logger.error(f"Hybrid {task_type} execution failed: {e}")
                    results.append({
                        'success': False,
                        'message': f'{task_type} execution failed: {str(e)}',
                        'task_type': task_type
                    })
        
        # è¯„ä¼°æ··åˆæ‰§è¡Œç»“æœ
        total_success = all(r.get('success', False) for r in results)
        
        return {
            'success': total_success,
            'message': f'Hybrid parallel execution completed',
            'results': results,
            'local_steps_count': len(local_steps),
            'remote_steps_count': len(remote_steps)
        }

    def _generate_gitlab_parallel_config(self, steps: List[AtomicStep], sync_policy: str) -> str:
        """
        ç”ŸæˆGitLab CIçš„å¹¶è¡Œæ‰§è¡Œé…ç½®
        """
        jobs = {}
        
        # ç”Ÿæˆå¹¶è¡Œä½œä¸š
        for step in steps:
            job_name = step.name.replace(' ', '_').replace('-', '_').lower()
            
            job_config = {
                'stage': 'parallel_execution',
                'script': self._convert_step_to_gitlab_script(step),
                'parallel': 1,  # æ ‡è®°ä¸ºå¹¶è¡Œä½œä¸š
            }
            
            # æ ¹æ®æ­¥éª¤ç±»å‹æ·»åŠ ç‰¹æ®Šé…ç½®
            if step.step_type == 'docker':
                job_config['image'] = step.config.get('image', 'ubuntu:latest')
            
            # æ·»åŠ å¤±è´¥ç­–ç•¥
            if sync_policy == 'fail_fast':
                job_config['allow_failure'] = False
            elif sync_policy == 'wait_any':
                job_config['allow_failure'] = True
            
            jobs[job_name] = job_config
        
        # æ„å»ºå®Œæ•´çš„GitLab CIé…ç½®
        gitlab_config = {
            'stages': ['parallel_execution'],
            **jobs
        }
        
        try:
            import yaml
            return yaml.dump(gitlab_config, default_flow_style=False)
        except ImportError:
            logger.warning("PyYAML not installed, returning JSON format")
            import json
            return json.dumps(gitlab_config, indent=2)
    
    def _generate_github_parallel_workflow(self, steps: List[AtomicStep], sync_policy: str) -> str:
        """
        ç”ŸæˆGitHub Actionsçš„å¹¶è¡Œworkflowé…ç½®
        """
        jobs = {}
        
        # ç”Ÿæˆå¹¶è¡Œä½œä¸š
        for step in steps:
            job_name = step.name.replace(' ', '_').replace('-', '_').lower()
            
            job_config = {
                'runs-on': 'ubuntu-latest',
                'steps': [
                    {
                        'name': 'Checkout code',
                        'uses': 'actions/checkout@v3'
                    },
                    {
                        'name': step.name,
                        'run': self._convert_step_to_github_script(step)
                    }
                ]
            }
            
            # æ ¹æ®æ­¥éª¤ç±»å‹æ·»åŠ ç‰¹æ®Šé…ç½®
            if step.step_type == 'docker':
                job_config['container'] = step.config.get('image', 'ubuntu:latest')
            
            # æ·»åŠ å¤±è´¥ç­–ç•¥
            if sync_policy != 'fail_fast':
                job_config['continue-on-error'] = True
            
            jobs[job_name] = job_config
        
        # æ„å»ºå®Œæ•´çš„GitHub Actions workflow
        workflow_config = {
            'name': 'Parallel Execution Workflow',
            'on': {
                'workflow_dispatch': {
                    'inputs': {}
                }
            },
            'jobs': jobs
        }
        
        try:
            import yaml
            return yaml.dump(workflow_config, default_flow_style=False)
        except ImportError:
            logger.warning("PyYAML not installed, returning JSON format")
            import json
            return json.dumps(workflow_config, indent=2)
    
    def _convert_step_to_gitlab_script(self, step: AtomicStep) -> List[str]:
        """
        å°†æ­¥éª¤è½¬æ¢ä¸ºGitLab CIè„šæœ¬
        """
        if step.step_type == 'shell':
            return [step.config.get("command", "echo 'No command specified'")]
        elif step.step_type == 'python':
            script = step.config.get("script", "print('Hello World')")
            return [f'python3 -c "{script}"']
        elif step.step_type == 'docker':
            command = step.config.get('command', 'echo "Hello World"')
            return [command]
        else:
            return [f'echo "Executing step: {step.name}"']
    
    def _convert_step_to_github_script(self, step: AtomicStep) -> str:
        """
        å°†æ­¥éª¤è½¬æ¢ä¸ºGitHub Actionsè„šæœ¬
        """
        if step.step_type == 'shell':
            return step.config.get("command", "echo 'No command specified'")
        elif step.step_type == 'python':
            script = step.config.get("script", "print('Hello World')")
            return f'python3 -c "{script}"'
        elif step.step_type == 'docker':
            command = step.config.get('command', 'echo "Hello World"')
            return command
        else:
            return f'echo "Executing step: {step.name}"'
    
    def _should_execute_locally_in_parallel(self, step: AtomicStep) -> bool:
        """
        åˆ¤æ–­æ­¥éª¤æ˜¯å¦åº”è¯¥åœ¨æœ¬åœ°å¹¶è¡Œæ‰§è¡Œï¼ˆæ··åˆæ¨¡å¼ä¸‹ï¼‰
        """
        # æœ¬åœ°æ‰§è¡Œä¼˜å…ˆçš„æ­¥éª¤ç±»å‹
        local_preferred_types = ['python', 'shell', 'file_operation']
        
        # è¿œç¨‹æ‰§è¡Œä¼˜å…ˆçš„æ­¥éª¤ç±»å‹
        remote_preferred_types = ['docker', 'kubernetes', 'terraform']
        
        if step.step_type in local_preferred_types:
            return True
        elif step.step_type in remote_preferred_types:
            return False
        else:
            # é»˜è®¤ç­–ç•¥ï¼šç®€å•æ­¥éª¤æœ¬åœ°æ‰§è¡Œï¼Œå¤æ‚æ­¥éª¤è¿œç¨‹æ‰§è¡Œ
            config_complexity = len(str(step.config))
            return config_complexity < 500  # é…ç½®ç®€å•çš„æœ¬åœ°æ‰§è¡Œ
    
    def _get_gitlab_service(self, tool):
        """
        è·å–GitLabæœåŠ¡å®ä¾‹
        """
        # è¿™é‡Œåº”è¯¥è¿”å›GitLabæœåŠ¡å®ä¾‹
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿå¯¹è±¡
        class MockGitLabService:
            def trigger_parallel_pipeline(self, project_id, gitlab_config, variables, ref):
                return {
                    'pipeline_id': 12345,
                    'web_url': f'https://gitlab.example.com/project/{project_id}/-/pipelines/12345'
                }
        
        return MockGitLabService()
    
    def _get_github_service(self, tool):
        """
        è·å–GitHubæœåŠ¡å®ä¾‹
        """
        # è¿™é‡Œåº”è¯¥è¿”å›GitHubæœåŠ¡å®ä¾‹
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿå¯¹è±¡
        class MockGitHubService:
            def trigger_parallel_workflow(self, owner, repo, workflow_config, inputs, ref):
                return {
                    'run_id': 67890,
                    'html_url': f'https://github.com/{owner}/{repo}/actions/runs/67890'
                }
        
        return MockGitHubService()

    def analyze_parallel_groups(self, steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä»æ­¥éª¤åˆ—è¡¨åˆ†æå¹¶è¡Œç»„
        
        Args:
            steps: æ­¥éª¤åˆ—è¡¨ï¼Œæ¯ä¸ªæ­¥éª¤åŒ…å« name, step_type, order, parallel_group ç­‰å­—æ®µ
            
        Returns:
            å¹¶è¡Œç»„åˆ—è¡¨ï¼Œæ¯ä¸ªç»„åŒ…å« name, order, steps å­—æ®µ
        """
        logger.info(f"analyze_parallel_groups å¼€å§‹åˆ†æï¼Œæ”¶åˆ° {len(steps)} ä¸ªæ­¥éª¤")
        for i, step in enumerate(steps):
            logger.info(f"æ­¥éª¤ {i}: name={step.get('name')}, parallel_group={step.get('parallel_group')}, order={step.get('order')}")
            
        if not steps:
            return []
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ­¥éª¤æ˜ç¡®æŒ‡å®šäº†parallel_group
        explicit_groups = {}
        remaining_steps = []
        
        for step in steps:
            parallel_group = step.get('parallel_group')
            if parallel_group:
                # æœ‰æ˜ç¡®æŒ‡å®šçš„å¹¶è¡Œç»„
                logger.info(f"å‘ç°å¹¶è¡Œç»„æ­¥éª¤: {step.get('name')} -> {parallel_group}")
                if parallel_group not in explicit_groups:
                    explicit_groups[parallel_group] = {
                        'id': parallel_group,
                        'name': f'parallel_group_{parallel_group}',
                        'steps': [],
                        'order': step.get('order', 0),
                        'parallel': True,
                        'sync_policy': 'wait_all'
                    }
                explicit_groups[parallel_group]['steps'].append(step)
                # æ›´æ–°orderä¸ºæœ€å°å€¼
                explicit_groups[parallel_group]['order'] = min(
                    explicit_groups[parallel_group]['order'], 
                    step.get('order', 0)
                )
            else:
                logger.info(f"é¡ºåºæ­¥éª¤: {step.get('name')} (æ— parallel_group)")
                remaining_steps.append(step)
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å¹¶è¡Œç»„ï¼Œåˆ™æŒ‰orderåˆ†ç»„ï¼ˆç›¸åŒorderçš„æ­¥éª¤å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼‰
        parallel_groups = list(explicit_groups.values())
        logger.info(f"æ‰¾åˆ°æ˜ç¡®çš„å¹¶è¡Œç»„: {len(parallel_groups)} ä¸ª")
        
        if not parallel_groups and remaining_steps:
            # æŒ‰orderåˆ†ç»„ï¼Œç›¸åŒorderçš„æ­¥éª¤å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
            order_groups = {}
            
            for step in remaining_steps:
                order = step.get('order', 0)
                if order not in order_groups:
                    order_groups[order] = []
                order_groups[order].append(step)
            
            # æ£€æŸ¥æ¯ä¸ªorderç»„ï¼Œå¦‚æœåŒ…å«å¤šä¸ªæ­¥éª¤ï¼Œåˆ™åˆ›å»ºå¹¶è¡Œç»„
            for order, group_steps in order_groups.items():
                if len(group_steps) > 1:
                    # åˆ›å»ºå¹¶è¡Œç»„
                    parallel_group = {
                        'id': f'auto_parallel_order_{order}',
                        'name': f'parallel_group_order_{order}',
                        'order': order,
                        'steps': group_steps,
                        'parallel': True,
                        'sync_policy': 'wait_all'  # é»˜è®¤ç­‰å¾…æ‰€æœ‰æ­¥éª¤å®Œæˆ
                    }
                    parallel_groups.append(parallel_group)
        
        logger.info(f"åˆ†æåˆ° {len(parallel_groups)} ä¸ªå¹¶è¡Œç»„ï¼Œæ€»æ­¥éª¤æ•° {len(steps)}")
        for group in parallel_groups:
            logger.info(f"å¹¶è¡Œç»„ {group['name']}: {len(group['steps'])} ä¸ªæ­¥éª¤, order={group['order']}")
        
        return parallel_groups
    
    def execute_pipeline_with_performance_optimization(self, 
                                                     pipeline: Pipeline, 
                                                     pipeline_run: PipelineRun, 
                                                     execution_plan: Dict[str, Any],
                                                     optimization_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œæµæ°´çº¿ï¼ŒåŒ…å«æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
        
        Args:
            pipeline: æµæ°´çº¿å¯¹è±¡
            pipeline_run: æµæ°´çº¿è¿è¡Œå®ä¾‹
            execution_plan: æ‰§è¡Œè®¡åˆ’
            optimization_config: ä¼˜åŒ–é…ç½®
                - batch_size: æ‰¹å¤„ç†å¤§å°
                - adaptive_workers: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”å·¥ä½œçº¿ç¨‹
                - memory_threshold_mb: å†…å­˜é˜ˆå€¼(MB)
                - enable_caching: æ˜¯å¦å¯ç”¨ç¼“å­˜
                - priority_scheduling: æ˜¯å¦å¯ç”¨ä¼˜å…ˆçº§è°ƒåº¦
        """
        if optimization_config is None:
            optimization_config = {
                'batch_size': 5,
                'adaptive_workers': True,
                'memory_threshold_mb': 1024,
                'enable_caching': True,
                'priority_scheduling': True
            }
        
        logger.info(f"Starting optimized pipeline {pipeline.id} execution")
        logger.info(f"Optimization config: {optimization_config}")
        
        # æ€§èƒ½ç›‘æ§
        start_time = timezone.now()
        performance_metrics = {
            'start_time': start_time,
            'stages_completed': 0,
            'parallel_groups_executed': 0,
            'total_steps_executed': 0,
            'memory_peaks': [],
            'execution_times': []
        }
        
        try:
            # åˆ›å»ºæµæ°´çº¿æ‰§è¡Œè®°å½•
            pipeline_execution = self._create_pipeline_execution(pipeline, pipeline_run)
            
            # è‡ªé€‚åº”å·¥ä½œçº¿ç¨‹æ± é…ç½®
            if optimization_config.get('adaptive_workers', True):
                optimal_workers = self._calculate_optimal_workers(execution_plan)
                self.max_parallel_workers = optimal_workers
                logger.info(f"Adaptive workers: using {optimal_workers} threads")
            
            # èµ„æºæ± åˆå§‹åŒ–
            resource_pool = self._initialize_resource_pool(optimization_config)
            
            # æŒ‰é˜¶æ®µæ‰§è¡Œï¼Œæ”¯æŒæ‰¹å¤„ç†
            stages = execution_plan['stages']
            batch_size = optimization_config.get('batch_size', 5)
            
            # å°†å¤§å‹å¹¶è¡Œç»„åˆ†æ‰¹å¤„ç†
            optimized_stages = self._optimize_stage_batching(stages, batch_size)
            
            for stage_batch in optimized_stages:
                batch_start_time = timezone.now()
                
                # å†…å­˜æ£€æŸ¥
                if self._check_memory_threshold(optimization_config.get('memory_threshold_mb', 1024)):
                    logger.warning("Memory threshold exceeded, triggering garbage collection")
                    self._perform_memory_cleanup()
                
                # æ‰§è¡Œæ‰¹æ¬¡
                batch_result = self._execute_stage_batch(
                    stage_batch, 
                    pipeline, 
                    pipeline_execution,
                    resource_pool,
                    optimization_config
                )
                
                if not batch_result['success']:
                    logger.error(f"Stage batch failed: {batch_result['message']}")
                    return {
                        'success': False,
                        'message': f"Pipeline failed: {batch_result['message']}",
                        'performance_metrics': performance_metrics
                    }
                
                # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                batch_end_time = timezone.now()
                performance_metrics['stages_completed'] += len(stage_batch)
                performance_metrics['execution_times'].append(
                    (batch_end_time - batch_start_time).total_seconds()
                )
                
                # è®°å½•å†…å­˜ä½¿ç”¨
                memory_usage = self._get_current_memory_usage()
                performance_metrics['memory_peaks'].append(memory_usage)
            
            # æµæ°´çº¿å®Œæˆ
            end_time = timezone.now()
            total_execution_time = (end_time - start_time).total_seconds()
            
            pipeline_execution.status = 'success'
            pipeline_execution.completed_at = end_time
            pipeline_execution.save()
            
            # æ›´æ–°æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
            performance_metrics.update({
                'end_time': end_time,
                'total_execution_time': total_execution_time,
                'avg_stage_time': statistics.mean(performance_metrics['execution_times']) if performance_metrics['execution_times'] else 0,
                'peak_memory_mb': max(performance_metrics['memory_peaks']) if performance_metrics['memory_peaks'] else 0
            })
            
            logger.info(f"Optimized pipeline completed in {total_execution_time:.2f}s")
            
            return {
                'success': True,
                'message': 'Pipeline completed successfully with performance optimization',
                'execution_id': pipeline_execution.id,
                'performance_metrics': performance_metrics
            }
            
        except Exception as e:
            logger.error(f"Optimized pipeline execution failed: {e}")
            if 'pipeline_execution' in locals():
                pipeline_execution.status = 'failed'
                pipeline_execution.completed_at = timezone.now()
                pipeline_execution.save()
            
            return {
                'success': False,
                'message': f'Optimized pipeline execution failed: {str(e)}',
                'performance_metrics': performance_metrics
            }
    
    def _calculate_optimal_workers(self, execution_plan: Dict[str, Any]) -> int:
        """è®¡ç®—æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°"""
        if PSUTIL_AVAILABLE:
            cpu_count = psutil.cpu_count()
        else:
            cpu_count = 4  # é»˜è®¤å€¼
        
        # åˆ†æå¹¶è¡Œç»„çš„å¤æ‚åº¦
        total_parallel_steps = 0
        max_group_size = 0
        
        for stage in execution_plan['stages']:
            if stage['parallel']:
                group_size = len(stage['items'])
                total_parallel_steps += group_size
                max_group_size = max(max_group_size, group_size)
        
        # è‡ªé€‚åº”ç®—æ³•
        if total_parallel_steps == 0:
            return min(4, cpu_count)  # é¡ºåºæ‰§è¡Œï¼Œå°‘é‡çº¿ç¨‹å³å¯
        
        # åŸºäºCPUæ ¸å¿ƒæ•°å’Œå¹¶è¡Œæ­¥éª¤æ•°è®¡ç®—
        optimal_workers = min(
            cpu_count * 2,  # ä¸è¶…è¿‡CPUæ ¸å¿ƒæ•°çš„2å€
            max_group_size + 2,  # åŸºäºæœ€å¤§å¹¶è¡Œç»„å¤§å°
            total_parallel_steps,  # ä¸è¶…è¿‡æ€»å¹¶è¡Œæ­¥éª¤æ•°
            20  # ç¡¬é™åˆ¶
        )
        
        return max(2, optimal_workers)  # è‡³å°‘2ä¸ªçº¿ç¨‹
    
    def _initialize_resource_pool(self, optimization_config: Dict[str, Any]) -> Dict[str, Any]:
        """åˆå§‹åŒ–èµ„æºæ± """
        return {
            'step_cache': {} if optimization_config.get('enable_caching', True) else None,
            'execution_history': [],
            'resource_locks': {},
            'priority_queue': [] if optimization_config.get('priority_scheduling', True) else None
        }
    
    def _optimize_stage_batching(self, stages: List[Dict[str, Any]], batch_size: int) -> List[List[Dict[str, Any]]]:
        """ä¼˜åŒ–é˜¶æ®µæ‰¹å¤„ç†ï¼Œå°†å¤§å‹å¹¶è¡Œç»„åˆ†æ‰¹"""
        optimized_batches = []
        current_batch = []
        
        for stage in stages:
            if stage['parallel'] and len(stage['items']) > batch_size:
                # å¤§å‹å¹¶è¡Œç»„åˆ†æ‰¹å¤„ç†
                if current_batch:
                    optimized_batches.append(current_batch)
                    current_batch = []
                
                # å°†å¤§å‹å¹¶è¡Œç»„æ‹†åˆ†ä¸ºå¤šä¸ªå°æ‰¹æ¬¡
                items = stage['items']
                for i in range(0, len(items), batch_size):
                    batch_items = items[i:i + batch_size]
                    sub_stage = {
                        **stage,
                        'items': batch_items,
                        'original_group_size': len(items),
                        'batch_index': i // batch_size,
                        'is_sub_batch': True
                    }
                    optimized_batches.append([sub_stage])
            else:
                current_batch.append(stage)
                
                # å¦‚æœå½“å‰æ‰¹æ¬¡å·²æ»¡ï¼Œå¼€å§‹æ–°æ‰¹æ¬¡
                if len(current_batch) >= batch_size:
                    optimized_batches.append(current_batch)
                    current_batch = []
        
        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            optimized_batches.append(current_batch)
        
        return optimized_batches
    
    def _execute_stage_batch(self, 
                           stage_batch: List[Dict[str, Any]], 
                           pipeline: Pipeline, 
                           pipeline_execution,
                           resource_pool: Dict[str, Any],
                           optimization_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé˜¶æ®µæ‰¹æ¬¡"""
        try:
            for stage in stage_batch:
                stage_result = self._execute_stage_optimized(
                    stage, 
                    pipeline, 
                    pipeline_execution,
                    resource_pool,
                    optimization_config
                )
                
                if not stage_result['success']:
                    return stage_result
            
            return {'success': True, 'message': 'Stage batch completed successfully'}
            
        except Exception as e:
            logger.error(f"Stage batch execution failed: {e}")
            return {'success': False, 'message': f'Stage batch failed: {str(e)}'}
    
    def _execute_stage_optimized(self, 
                               stage: Dict[str, Any], 
                               pipeline: Pipeline, 
                               pipeline_execution,
                               resource_pool: Dict[str, Any],
                               optimization_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¼˜åŒ–çš„é˜¶æ®µ"""
        if stage['parallel']:
            return self._execute_parallel_stage_optimized(
                stage, pipeline, pipeline_execution, resource_pool, optimization_config
            )
        else:
            return self._execute_sequential_stage_optimized(
                stage, pipeline, pipeline_execution, resource_pool, optimization_config
            )
    
    def _execute_parallel_stage_optimized(self, 
                                        stage: Dict[str, Any], 
                                        pipeline: Pipeline, 
                                        pipeline_execution,
                                        resource_pool: Dict[str, Any],
                                        optimization_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¼˜åŒ–çš„å¹¶è¡Œé˜¶æ®µ"""
        steps = stage['items']
        group_info = stage.get('group_info', {})
        
        logger.info(f"Executing optimized parallel stage with {len(steps)} steps")
        
        # ä¼˜å…ˆçº§è°ƒåº¦
        if optimization_config.get('priority_scheduling', True):
            steps = self._sort_steps_by_priority(steps)
        
        # ä½¿ç”¨ä¼˜åŒ–çš„çº¿ç¨‹æ± 
        with ThreadPoolExecutor(max_workers=self.max_parallel_workers) as executor:
            # æäº¤æ‰€æœ‰æ­¥éª¤
            futures = {}
            for step in steps:
                # æ£€æŸ¥ç¼“å­˜
                if self._check_step_cache(step, resource_pool):
                    continue
                
                future = executor.submit(
                    self._execute_step_with_monitoring, 
                    step, 
                    pipeline, 
                    pipeline_execution,
                    resource_pool
                )
                futures[future] = step
            
            # æ”¶é›†ç»“æœ
            successful_steps = 0
            failed_steps = []
            
            for future in as_completed(futures, timeout=group_info.get('timeout_seconds', 300)):
                step = futures[future]
                try:
                    result = future.result()
                    if result['success']:
                        successful_steps += 1
                        # æ›´æ–°ç¼“å­˜
                        self._update_step_cache(step, result, resource_pool)
                    else:
                        failed_steps.append((step, result['message']))
                        
                except Exception as e:
                    logger.error(f"Step {step.get('name', 'unknown')} failed: {e}")
                    failed_steps.append((step, str(e)))
            
            # æ ¹æ®åŒæ­¥ç­–ç•¥åˆ¤æ–­æˆåŠŸ
            sync_policy = group_info.get('sync_policy', 'wait_all')
            if sync_policy == 'wait_all' and failed_steps:
                return {
                    'success': False,
                    'message': f'Parallel stage failed: {len(failed_steps)} steps failed'
                }
            elif sync_policy == 'wait_any' and successful_steps == 0:
                return {
                    'success': False,
                    'message': 'Parallel stage failed: no steps succeeded'
                }
            
            return {
                'success': True,
                'message': f'Parallel stage completed: {successful_steps} succeeded, {len(failed_steps)} failed'
            }
    
    def _execute_sequential_stage_optimized(self, 
                                          stage: Dict[str, Any], 
                                          pipeline: Pipeline, 
                                          pipeline_execution,
                                          resource_pool: Dict[str, Any],
                                          optimization_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¼˜åŒ–çš„é¡ºåºé˜¶æ®µ"""
        steps = stage['items']
        
        for step in steps:
            # æ£€æŸ¥ç¼“å­˜
            if self._check_step_cache(step, resource_pool):
                continue
            
            result = self._execute_step_with_monitoring(
                step, 
                pipeline, 
                pipeline_execution,
                resource_pool
            )
            
            if not result['success']:
                return {
                    'success': False,
                    'message': f'Sequential step failed: {result["message"]}'
                }
            
            # æ›´æ–°ç¼“å­˜
            self._update_step_cache(step, result, resource_pool)
        
        return {'success': True, 'message': 'Sequential stage completed'}
    
    def _execute_step_with_monitoring(self, 
                                    step: Any, 
                                    pipeline: Pipeline, 
                                    pipeline_execution,
                                    resource_pool: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ­¥éª¤å¹¶ç›‘æ§æ€§èƒ½"""
        step_start_time = time.time()
        
        try:
            # æ‰§è¡Œæ­¥éª¤ï¼ˆè¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ­¥éª¤æ‰§è¡Œé€»è¾‘ï¼‰
            result = self.local_executor.execute_step(step, {})
            
            # è®°å½•æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - step_start_time
            resource_pool['execution_history'].append({
                'step_id': getattr(step, 'id', 'unknown'),
                'execution_time': execution_time,
                'success': result.get('success', False),
                'timestamp': timezone.now()
            })
            
            return result
            
        except Exception as e:
            logger.error(f"Step monitoring failed: {e}")
            return {'success': False, 'message': str(e)}
    
    def _sort_steps_by_priority(self, steps: List[Any]) -> List[Any]:
        """æ ¹æ®ä¼˜å…ˆçº§æ’åºæ­¥éª¤"""
        def get_priority(step):
            # åŸºäºé¢„ä¼°æ‰§è¡Œæ—¶é—´å’Œä¾èµ–å…³ç³»è®¡ç®—ä¼˜å…ˆçº§
            estimated_time = getattr(step, 'estimated_duration', 0) or 0
            dependencies_count = len(getattr(step, 'dependencies', []))
            
            # çŸ­ä½œä¸šä¼˜å…ˆ + ä¾èµ–å…³ç³»è€ƒè™‘
            return estimated_time + dependencies_count * 10
        
        return sorted(steps, key=get_priority)
    
    def _check_step_cache(self, step: Any, resource_pool: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ­¥éª¤ç¼“å­˜"""
        if not resource_pool.get('step_cache'):
            return False
        
        step_id = getattr(step, 'id', None)
        if not step_id:
            return False
        
        # ç®€å•çš„ç¼“å­˜ç­–ç•¥ï¼šåŸºäºæ­¥éª¤IDå’Œå‚æ•°hash
        step_hash = self._calculate_step_hash(step)
        return step_hash in resource_pool['step_cache']
    
    def _update_step_cache(self, step: Any, result: Dict[str, Any], resource_pool: Dict[str, Any]):
        """æ›´æ–°æ­¥éª¤ç¼“å­˜"""
        if not resource_pool.get('step_cache'):
            return
        
        step_hash = self._calculate_step_hash(step)
        resource_pool['step_cache'][step_hash] = {
            'result': result,
            'timestamp': timezone.now(),
            'step_name': getattr(step, 'name', 'unknown')
        }
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(resource_pool['step_cache']) > 1000:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = min(
                resource_pool['step_cache'].keys(),
                key=lambda k: resource_pool['step_cache'][k]['timestamp']
            )
            del resource_pool['step_cache'][oldest_key]
    
    def _calculate_step_hash(self, step: Any) -> str:
        """è®¡ç®—æ­¥éª¤hash"""
        import hashlib
        
        step_data = {
            'id': getattr(step, 'id', ''),
            'name': getattr(step, 'name', ''),
            'type': getattr(step, 'type', ''),
            'parameters': getattr(step, 'parameters', {})
        }
        
        step_str = json.dumps(step_data, sort_keys=True)
        return hashlib.md5(step_str.encode()).hexdigest()
    
    def _check_memory_threshold(self, threshold_mb: int) -> bool:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æ˜¯å¦è¶…è¿‡é˜ˆå€¼"""
        if not PSUTIL_AVAILABLE:
            return False
            
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            return memory_mb > threshold_mb
        except Exception:
            return False
    
    def _perform_memory_cleanup(self):
        """æ‰§è¡Œå†…å­˜æ¸…ç†"""
        gc.collect()
        logger.info("Memory cleanup performed")
    
    def _get_current_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        if not PSUTIL_AVAILABLE:
            return 0.0
            
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except Exception:
            return 0.0

    def analyze_pipeline_step_execution_plan(self, pipeline: Pipeline) -> Dict[str, Any]:
        """
        åˆ†ææµæ°´çº¿çš„PipelineStepæ‰§è¡Œè®¡åˆ’ï¼Œè¯†åˆ«å¹¶è¡Œç»„å’Œä¾èµ–å…³ç³»
        
        Returns:
            æ‰§è¡Œè®¡åˆ’ï¼ŒåŒ…å«æ­¥éª¤åˆ†ç»„ã€ä¾èµ–å…³ç³»å’Œæ‰§è¡Œé¡ºåº
        """
        steps = list(pipeline.steps.all().order_by('order'))
        parallel_groups = {}
        sequential_steps = []
        execution_plan = {
            'stages': [],  # æ‰§è¡Œé˜¶æ®µåˆ—è¡¨
            'parallel_groups': {},
            'dependencies': {},
            'total_stages': 0
        }
        
        # æ”¶é›†å¹¶è¡Œç»„ä¿¡æ¯
        for step in steps:
            if hasattr(step, 'parallel_group') and step.parallel_group:
                group_id = step.parallel_group
                if group_id not in parallel_groups:
                    parallel_groups[group_id] = {
                        'id': group_id,
                        'name': f"å¹¶è¡Œç»„-{group_id[:8]}",
                        'steps': [],
                        'sync_policy': 'wait_all',  # é»˜è®¤ç­‰å¾…æ‰€æœ‰æ­¥éª¤å®Œæˆ
                        'timeout_seconds': None,
                        'min_order': float('inf'),
                        'max_order': 0
                    }
                
                parallel_groups[group_id]['steps'].append(step)
                parallel_groups[group_id]['min_order'] = min(parallel_groups[group_id]['min_order'], step.order)
                parallel_groups[group_id]['max_order'] = max(parallel_groups[group_id]['max_order'], step.order)
                logger.info(f"Added PipelineStep {step.name} to parallel group {group_id} with order {step.order}")
            else:
                sequential_steps.append(step)
                logger.info(f"Added PipelineStep {step.name} to sequential execution with order {step.order}")
        
        logger.info(f"Found {len(parallel_groups)} parallel groups and {len(sequential_steps)} sequential steps")
        
        # åˆ›å»ºæ‰§è¡Œé˜¶æ®µ
        # æŒ‰ç…§ order å­—æ®µæ’åºï¼Œå°†å¹¶è¡Œç»„å’Œå•ç‹¬æ­¥éª¤å®‰æ’åˆ°ä¸åŒçš„é˜¶æ®µ
        all_items = []
        
        # æ·»åŠ å•ç‹¬æ­¥éª¤
        for step in sequential_steps:
            all_items.append({
                'type': 'step',
                'order': step.order,
                'item': step,
                'parallel': False
            })
        
        # æ·»åŠ å¹¶è¡Œç»„ï¼ˆä½¿ç”¨æœ€å°orderä½œä¸ºç»„çš„orderï¼‰
        for group_id, group_info in parallel_groups.items():
            all_items.append({
                'type': 'parallel_group',
                'order': group_info['min_order'],
                'item': group_info,
                'parallel': True
            })
        
        # æŒ‰orderæ’åº
        all_items.sort(key=lambda x: x['order'])
        
        # æ„å»ºæ‰§è¡Œé˜¶æ®µ
        stage_number = 0
        for item in all_items:
            stage = {
                'stage_number': stage_number,
                'type': item['type'],
                'parallel': item['parallel'],
                'items': []
            }
            
            if item['type'] == 'step':
                stage['items'] = [item['item']]
            else:  # parallel_group
                stage['items'] = item['item']['steps']
                stage['group_info'] = item['item']
            
            execution_plan['stages'].append(stage)
            stage_number += 1
        
        execution_plan['total_stages'] = stage_number
        execution_plan['parallel_groups'] = parallel_groups
        
        logger.info(f"Pipeline {pipeline.id} PipelineStep execution plan: {stage_number} stages, {len(parallel_groups)} parallel groups")
        return execution_plan

    def execute_pipeline_step_with_parallel_support(self, 
                                                    pipeline: Pipeline, 
                                                    pipeline_run, 
                                                    execution_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®æ‰§è¡Œè®¡åˆ’æ‰§è¡Œæµæ°´çº¿çš„PipelineStepï¼Œæ”¯æŒå¹¶è¡Œç»„
        """
        logger.info(f"Starting PipelineStep execution for pipeline {pipeline.id} with parallel support")
        
        try:
            # ğŸ”¥ åˆ›å»ºå…±äº«å·¥ä½œç©ºé—´çŠ¶æ€ï¼ˆä¸ºPipelineStepæ‰§è¡Œï¼‰
            from cicd_integrations.executors.execution_context import ExecutionContext
            
            execution_context = ExecutionContext(
                execution_id=pipeline_run.id,
                pipeline_name=pipeline.name,
                trigger_type='manual'
            )
            
            # å…±äº«çš„å·¥ä½œç›®å½•çŠ¶æ€
            shared_workspace_state = {
                'working_directory': execution_context.get_workspace_path(),
                'execution_id': pipeline_run.id,
                'pipeline_name': pipeline.name
            }
            
            logger.info(f"ğŸ  åˆ›å»ºPipelineStepå…±äº«å·¥ä½œç©ºé—´: {shared_workspace_state['working_directory']}")
            
            # æŒ‰é˜¶æ®µæ‰§è¡Œ
            for stage in execution_plan['stages']:
                stage_result = self._execute_pipeline_step_stage(
                    stage, 
                    pipeline, 
                    pipeline_run, 
                    execution_plan,
                    shared_workspace_state  # ä¼ é€’å…±äº«çŠ¶æ€
                )
                
                if not stage_result['success']:
                    logger.error(f"PipelineStep stage {stage['stage_number']} failed: {stage_result['message']}")
                    return {
                        'success': False,
                        'message': f"Pipeline failed at stage {stage['stage_number']}: {stage_result['message']}",
                        'failed_stage': stage['stage_number']
                    }
            
            # æ‰€æœ‰é˜¶æ®µå®Œæˆ
            return {
                'success': True,
                'message': 'Pipeline PipelineStep execution completed successfully',
                'execution_type': 'pipeline_step'
            }
            
        except Exception as e:
            logger.error(f"PipelineStep execution failed: {e}")
            return {
                'success': False,
                'message': f'PipelineStep execution failed: {str(e)}'
            }

    def _execute_pipeline_step_stage(self, 
                                    stage: Dict[str, Any], 
                                    pipeline: Pipeline, 
                                    pipeline_run,
                                    execution_plan: Dict[str, Any],
                                    shared_workspace_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªPipelineStepé˜¶æ®µï¼ˆå¯èƒ½åŒ…å«å¹¶è¡Œæ­¥éª¤ï¼‰
        """
        stage_number = stage['stage_number']
        is_parallel = stage['parallel']
        steps = stage['items']
        
        logger.info(f"å¼€å§‹æ‰§è¡ŒPipelineStepé˜¶æ®µ {stage_number}: {'å¹¶è¡Œ' if is_parallel else 'ä¸²è¡Œ'} æ‰§è¡Œï¼ŒåŒ…å« {len(steps)} ä¸ªæ­¥éª¤")
        
        if is_parallel:
            # å¹¶è¡Œæ‰§è¡Œ
            return self._execute_parallel_pipeline_steps(steps, stage.get('group_info', {}), pipeline_run, shared_workspace_state)
        else:
            # ä¸²è¡Œæ‰§è¡Œ
            return self._execute_sequential_pipeline_steps(steps, pipeline_run, shared_workspace_state)

    def _execute_parallel_pipeline_steps(self, steps: List, group_info: Dict[str, Any], pipeline_execution, shared_workspace_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªPipelineStep
        """
        import subprocess
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        logger.info(f"å¼€å§‹æœ¬åœ°å¹¶è¡Œæ‰§è¡Œ {len(steps)} ä¸ªPipelineStepï¼ŒåŒæ­¥ç­–ç•¥: {group_info.get('sync_policy', 'wait_all')}")
        
        # åˆ›å»ºçº¿ç¨‹æ± 
        max_workers = min(len(steps), self.max_parallel_workers)
        logger.info(f"åˆ›å»ºçº¿ç¨‹æ± ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
        
        def execute_single_pipeline_step(step):
            """æ‰§è¡Œå•ä¸ªPipelineStep"""
            logger.info(f"å¼€å§‹æœ¬åœ°æ‰§è¡ŒPipelineStep: {step.name}")
            
            try:
                # é¦–å…ˆåˆ›å»ºStepExecutionè®°å½•
                from cicd_integrations.models import StepExecution
                
                step_execution = StepExecution.objects.create(
                    pipeline_execution=pipeline_execution,
                    pipeline_step=step,
                    status='running',
                    order=step.order,
                    started_at=timezone.now()
                )
                
                # æ›´æ–°æ­¥éª¤çŠ¶æ€
                step.status = 'running'
                step.started_at = timezone.now()
                step.save()
                
                # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡è·å–å·¥ä½œç›®å½•
                from cicd_integrations.executors.execution_context import ExecutionContext
                
                execution_context = ExecutionContext(
                    execution_id=pipeline_execution.id,
                    pipeline_name=pipeline_execution.pipeline.name,
                    trigger_type='manual'
                )
                
                working_directory = execution_context.get_workspace_path()
                logger.info(f"PipelineStep {step.name} å·¥ä½œç›®å½•: {working_directory}")
                
                # ä½¿ç”¨LocalPipelineExecutoræ‰§è¡Œæ­¥éª¤ï¼Œæ”¯æŒå„ç§step_type
                context = {
                    'working_directory': working_directory,
                    'execution_id': pipeline_execution.id,
                    'pipeline_name': pipeline_execution.pipeline.name
                }
                
                # æ‰§è¡Œæ­¥éª¤ - æ”¯æŒfetch_codeã€docker_buildã€docker_pushç­‰å„ç§ç±»å‹
                from pipelines.services.local_executor import LocalPipelineExecutor
                local_executor = LocalPipelineExecutor()
                result = local_executor.execute_step(step, context)
                
                # æ ¹æ®æ‰§è¡Œç»“æœæ›´æ–°æ­¥éª¤çŠ¶æ€
                if result.get('success', False):
                    step.status = 'success'
                    step.output_log = result.get('output', '')
                    step_execution.status = 'success'
                    step_execution.logs = result.get('output', '')
                    step_execution.output = result.get('data', {})
                    logger.info(f"PipelineStep {step.name} æ‰§è¡Œå®Œæˆï¼Œç»“æœ: æˆåŠŸ")
                else:
                    step.status = 'failed'
                    step.error_log = result.get('error', 'Unknown error')
                    step_execution.status = 'failed'
                    step_execution.logs = result.get('error', 'Unknown error')
                    step_execution.error_message = result.get('error', 'Unknown error')
                    step_execution.output = result.get('data', {})
                    logger.error(f"PipelineStep {step.name} æ‰§è¡Œå¤±è´¥: {result.get('error', 'Unknown error')}")
                
                step.completed_at = timezone.now()
                step.save()
                
                step_execution.completed_at = timezone.now()
                step_execution.save()
                
                return {
                    'success': step.status == 'success',
                    'step_name': step.name,
                    'output': step.output_log,
                    'error': step.error_log if step.status == 'failed' else None
                }
                
            except Exception as e:
                logger.error(f"PipelineStep {step.name} æ‰§è¡Œå¼‚å¸¸: {e}")
                step.status = 'failed'
                step.error_log = str(e)
                step.completed_at = timezone.now()
                step.save()
                
                # æ›´æ–°StepExecutionè®°å½•
                if 'step_execution' in locals():
                    step_execution.status = 'failed'
                    step_execution.error_message = str(e)
                    step_execution.logs = str(e)
                    step_execution.completed_at = timezone.now()
                    step_execution.save()
                
                return {
                    'success': False,
                    'step_name': step.name,
                    'error': str(e)
                }
        
        # å¹¶è¡Œæ‰§è¡Œæ­¥éª¤
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            futures = []
            for step in steps:
                future = executor.submit(execute_single_pipeline_step, step)
                futures.append(future)
                logger.info(f"  - PipelineStep: {step.name}, é¡ºåº: {step.order}, ç±»å‹: {step.step_type}")
            
            logger.info(f"å·²æäº¤ {len(futures)} ä¸ªå¹¶è¡Œä»»åŠ¡åˆ°çº¿ç¨‹æ± ")
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = []
            success_count = 0
            failed_count = 0
            
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
                
                if result['success']:
                    success_count += 1
                    logger.info(f"PipelineStep {result['step_name']} æ‰§è¡ŒæˆåŠŸ")
                else:
                    failed_count += 1
                    logger.error(f"PipelineStep {result['step_name']} æ‰§è¡Œå¤±è´¥: {result.get('error', '')}")
        
        # æ ¹æ®åŒæ­¥ç­–ç•¥åˆ¤æ–­æ•´ä½“ç»“æœ
        sync_policy = group_info.get('sync_policy', 'wait_all')
        overall_success = (success_count > 0) if sync_policy == 'wait_any' else (failed_count == 0)
        
        logger.info(f"å¹¶è¡Œæ‰§è¡Œå®Œæˆ: {success_count} æˆåŠŸ, {failed_count} å¤±è´¥, æ€»ä½“ç»“æœ: {'æˆåŠŸ' if overall_success else 'å¤±è´¥'}")
        
        return {
            'success': overall_success,
            'message': f'Parallel execution completed: {success_count} success, {failed_count} failed',
            'results': results,
            'sync_policy': sync_policy
        }

    def _execute_sequential_pipeline_steps(self, steps: List, pipeline_execution, shared_workspace_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸²è¡Œæ‰§è¡Œå¤šä¸ªPipelineStep
        """
        import subprocess
        
        logger.info(f"Executing sequential stage with {len(steps)} steps")
        
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨å…±äº«çš„å·¥ä½œç›®å½•çŠ¶æ€ï¼Œè€Œä¸æ˜¯é‡æ–°åˆ›å»ºExecutionContext
        current_working_directory = shared_workspace_state['working_directory']
        logger.info(f"ğŸ  ä½¿ç”¨å…±äº«å·¥ä½œç›®å½•: {current_working_directory}")
        
        for step in steps:
            logger.info(f"å¼€å§‹æœ¬åœ°æ‰§è¡ŒPipelineStep: {step.name}")
            # é—®é¢˜1ä¿®å¤ï¼šåœ¨æ‰§è¡Œå¼€å§‹å°±æ‰“å°å·¥ä½œç›®å½•
            logger.info(f"ï¿½ğŸš€ === {step.name} === å·¥ä½œç›®å½•: {current_working_directory}")
            try:
                from cicd_integrations.models import StepExecution
                step_execution = StepExecution.objects.create(
                    pipeline_execution=pipeline_execution,
                    pipeline_step=step,
                    status='running',
                    order=step.order,
                    started_at=timezone.now()
                )
                step.status = 'running'
                step.started_at = timezone.now()
                step.save()
                context = {
                    'working_directory': current_working_directory,
                    'execution_id': pipeline_execution.id,
                    'pipeline_name': pipeline_execution.pipeline.name
                }
                from pipelines.services.local_executor import LocalPipelineExecutor
                local_executor = LocalPipelineExecutor()
                result = local_executor.execute_step(step, context)
                if result.get('data', {}).get('working_directory'):
                    new_working_directory = result['data']['working_directory']
                    if new_working_directory != current_working_directory:
                        logger.info(f"ğŸ”„ å·¥ä½œç›®å½•å·²æ›´æ–°: {current_working_directory} -> {new_working_directory}")
                        current_working_directory = new_working_directory
                if result.get('success', False):
                    step.status = 'success'
                    step.output_log = result.get('output', '')
                    step_execution.status = 'success'
                    step_execution.logs = result.get('output', '')
                    step_execution.output = result.get('data', {})
                    logger.info(f"PipelineStep {step.name} æ‰§è¡Œå®Œæˆï¼Œç»“æœ: æˆåŠŸ")
                else:
                    step.status = 'failed'
                    step.error_log = result.get('error', 'Unknown error')
                    step_execution.status = 'failed'
                    step_execution.logs = result.get('error', 'Unknown error')
                    step_execution.error_message = result.get('error', 'Unknown error')
                    step_execution.output = result.get('data', {})
                    logger.error(f"PipelineStep {step.name} æ‰§è¡Œå¤±è´¥: {result.get('error', 'Unknown error')}")
                    step.completed_at = timezone.now()
                    step.save()
                    step_execution.completed_at = timezone.now()
                    step_execution.save()
                    # é˜¶æ®µå¤±è´¥å‰åŒæ­¥ç›®å½•
                    shared_workspace_state['working_directory'] = current_working_directory
                    return {
                        'success': False,
                        'message': f'PipelineStep {step.name} failed',
                        'error': result.get('error', 'Unknown error')
                    }
                step.completed_at = timezone.now()
                step.save()
                step_execution.completed_at = timezone.now()
                step_execution.save()
            except Exception as e:
                logger.error(f"PipelineStep {step.name} æ‰§è¡Œå¼‚å¸¸: {e}")
                step.status = 'failed'
                step.error_log = str(e)
                step.completed_at = timezone.now()
                step.save()
                if 'step_execution' in locals():
                    step_execution.status = 'failed'
                    step_execution.error_message = str(e)
                    step_execution.logs = str(e)
                    step_execution.completed_at = timezone.now()
                    step_execution.save()
                # é˜¶æ®µå¼‚å¸¸å‰åŒæ­¥ç›®å½•
                shared_workspace_state['working_directory'] = current_working_directory
                return {
                    'success': False,
                    'message': f'PipelineStep {step.name} failed with exception',
                    'error': str(e)
                }
        # é˜¶æ®µæ­£å¸¸ç»“æŸååŒæ­¥ç›®å½•
        shared_workspace_state['working_directory'] = current_working_directory
        return {
            'success': True,
            'message': 'Sequential execution completed successfully'
        }

# å…¨å±€æœåŠ¡å®ä¾‹
parallel_execution_service = ParallelExecutionService()

#!/usr/bin/env python3
"""
AnsFlowç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿæ•°æ®ä¸€è‡´æ€§è¯Šæ–­è„šæœ¬
ç”¨äºåˆ†æå®æ—¶æ—¥å¿—ã€å†å²æŸ¥è¯¢å’Œæ—¥å¿—ç´¢å¼•çš„æ•°æ®æŠ“å–å¯¹è±¡ä¸€è‡´æ€§é—®é¢˜
"""

import os
import sys
import django
import asyncio
import json
from datetime import datetime, timedelta
from pathlib import Path

# è®¾ç½®Djangoç¯å¢ƒ
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ansflow.settings')
django.setup()

# ç°åœ¨å¯ä»¥å¯¼å…¥Djangoæ¨¡å—
from django.db import connection
import redis


class LogConsistencyDiagnostic:
    """æ—¥å¿—ä¸€è‡´æ€§è¯Šæ–­å™¨"""
    
    def __init__(self):
        self.redis_client = None
        self.setup_redis()
    
    def setup_redis(self):
        """è®¾ç½®Redisè¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host='localhost',
                port=6379,
                db=5,
                decode_responses=True
            )
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            print("âœ… Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
            self.redis_client = None
    
    def analyze_realtime_logs(self):
        """åˆ†æå®æ—¶æ—¥å¿—æ•°æ®æº"""
        print("\nğŸ” åˆ†æå®æ—¶æ—¥å¿—æ•°æ®æº...")
        
        if not self.redis_client:
            print("âŒ Redisä¸å¯ç”¨ï¼Œæ— æ³•åˆ†æå®æ—¶æ—¥å¿—")
            return
        
        try:
            # æ£€æŸ¥Redis Streamä¸­çš„æ—¥å¿—
            stream_info = self.redis_client.xinfo_stream('ansflow:logs:stream')
            total_messages = stream_info['length']
            print(f"ğŸ“Š Redis Streamæ€»æ¶ˆæ¯æ•°: {total_messages}")
            
            # è·å–æœ€è¿‘çš„æ¶ˆæ¯åˆ†ææœåŠ¡ç±»å‹
            messages = self.redis_client.xrevrange('ansflow:logs:stream', count=100)
            service_count = {}
            
            for msg_id, fields in messages:
                service = fields.get('service', 'unknown')
                service_count[service] = service_count.get(service, 0) + 1
            
            print("ğŸ“ˆ å®æ—¶æ—¥å¿—æœåŠ¡åˆ†å¸ƒ:")
            for service, count in service_count.items():
                print(f"   - {service}: {count} æ¡")
            
            # åˆ†æä¸ºä»€ä¹ˆFastAPIæ—¥å¿—ç¼ºå¤±
            if 'fastapi' not in service_count:
                print("âš ï¸  å‘ç°é—®é¢˜: FastAPIæœåŠ¡æ—¥å¿—ç¼ºå¤±")
                self.analyze_fastapi_logging_issue()
            
        except Exception as e:
            print(f"âŒ åˆ†æå®æ—¶æ—¥å¿—å¤±è´¥: {e}")
    
    def analyze_fastapi_logging_issue(self):
        """åˆ†æFastAPIæ—¥å¿—ç¼ºå¤±é—®é¢˜"""
        print("\nğŸ”§ åˆ†æFastAPIæ—¥å¿—ç¼ºå¤±åŸå› ...")
        
        # æ£€æŸ¥FastAPIæ—¥å¿—æ–‡ä»¶
        fastapi_log_dir = Path("/Users/creed/Workspace/OpenSource/ansflow/logs/services/fastapi")
        if fastapi_log_dir.exists():
            log_files = list(fastapi_log_dir.glob("*.log"))
            print(f"ğŸ“ FastAPIæ—¥å¿—æ–‡ä»¶: {[f.name for f in log_files]}")
            
            # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å†…å®¹
            for log_file in log_files[:3]:  # åªæ£€æŸ¥å‰3ä¸ªæ–‡ä»¶
                if log_file.stat().st_size > 0:
                    print(f"ğŸ“„ {log_file.name}: {log_file.stat().st_size} å­—èŠ‚")
                else:
                    print(f"ğŸ“„ {log_file.name}: ç©ºæ–‡ä»¶")
        else:
            print("âŒ FastAPIæ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
        
        # æ£€æŸ¥FastAPIæ˜¯å¦æœ‰ç›´æ¥å†™å…¥Redisçš„é…ç½®
        fastapi_service_dir = Path("/Users/creed/Workspace/OpenSource/ansflow/backend/fastapi_service")
        logging_integration_file = fastapi_service_dir / "logging_integration.py"
        
        if logging_integration_file.exists():
            with open(logging_integration_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if 'Redis' in content and 'xadd' in content:
                    print("âœ… FastAPIæœ‰Redisæ—¥å¿—é›†æˆä»£ç ")
                else:
                    print("âš ï¸  FastAPIç¼ºå°‘Redisç›´æ¥å†™å…¥åŠŸèƒ½")
        
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("   1. åœ¨FastAPI logging_integration.pyä¸­æ·»åŠ DirectRedisHandler")
        print("   2. ç¡®ä¿FastAPIæ—¥å¿—é€šè¿‡Redis Streamå†™å…¥")
        print("   3. æ£€æŸ¥WebSocketæ—¥å¿—ç›‘æ§æ˜¯å¦åŒ…å«FastAPIæ—¥å¿—æ–‡ä»¶")
    
    def analyze_historical_logs(self):
        """åˆ†æå†å²æ—¥å¿—æ•°æ®æº"""
        print("\nğŸ” åˆ†æå†å²æ—¥å¿—æ•°æ®æº...")
        
        try:
            with connection.cursor() as cursor:
                # æ£€æŸ¥æ˜¯å¦æœ‰unified_logsè¡¨
                cursor.execute("SHOW TABLES LIKE 'unified_logs'")
                table_exists = cursor.fetchone()
                
                if table_exists:
                    print("âœ… unified_logsè¡¨å­˜åœ¨")
                    
                    # ç»Ÿè®¡æ€»æ•°
                    cursor.execute("SELECT COUNT(*) FROM unified_logs")
                    total_count = cursor.fetchone()[0]
                    print(f"ğŸ“Š å†å²æ—¥å¿—æ€»æ•°: {total_count}")
                    
                    # æŒ‰æœåŠ¡ç»Ÿè®¡
                    cursor.execute("SELECT service, COUNT(*) FROM unified_logs GROUP BY service")
                    service_stats = cursor.fetchall()
                    
                    print("ğŸ“ˆ å†å²æ—¥å¿—æœåŠ¡åˆ†å¸ƒ:")
                    for service, count in service_stats:
                        print(f"   - {service}: {count} æ¡")
                    
                    # æ£€æŸ¥æœ€è¿‘çš„æ—¥å¿—
                    cursor.execute("""
                        SELECT service, level, message, timestamp 
                        FROM unified_logs 
                        ORDER BY timestamp DESC 
                        LIMIT 5
                    """)
                    recent_logs = cursor.fetchall()
                    
                    print("ğŸ• æœ€è¿‘5æ¡å†å²æ—¥å¿—:")
                    for service, level, message, timestamp in recent_logs:
                        print(f"   [{timestamp}] {service}.{level}: {message[:50]}...")
                    
                    # åˆ†æä¸ºä»€ä¹ˆåªæœ‰24æ¡è®°å½•
                    if total_count == 24:
                        print("âš ï¸  å‘ç°é—®é¢˜: å†å²æ—¥å¿—åªæœ‰24æ¡ï¼Œå¯èƒ½æ˜¯æµ‹è¯•æ•°æ®")
                        self.analyze_historical_log_pipeline()
                else:
                    print("âŒ unified_logsè¡¨ä¸å­˜åœ¨")
                    self.check_alternative_log_tables()
                
        except Exception as e:
            print(f"âŒ åˆ†æå†å²æ—¥å¿—å¤±è´¥: {e}")
    
    def analyze_historical_log_pipeline(self):
        """åˆ†æå†å²æ—¥å¿—æ•°æ®æµæ°´çº¿"""
        print("\nğŸ”§ åˆ†æå†å²æ—¥å¿—æ•°æ®æµæ°´çº¿...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªåŠ¨åŒ–çš„æ—¥å¿—å­˜å‚¨æœºåˆ¶
        print("ğŸ” æ£€æŸ¥æ—¥å¿—å­˜å‚¨æœºåˆ¶:")
        print("   1. æ˜¯å¦æœ‰å®šæ—¶ä»»åŠ¡å°†Redis Streamæ•°æ®è½¬å­˜åˆ°MySQL?")
        print("   2. æ˜¯å¦æœ‰æ—¥å¿—è½®è½¬å’Œå½’æ¡£æœºåˆ¶?")
        print("   3. åº”ç”¨æ—¥å¿—æ˜¯å¦ç›´æ¥å†™å…¥MySQL?")
        
        # æ£€æŸ¥Djangoæ—¥å¿—é…ç½®
        from django.conf import settings
        if hasattr(settings, 'LOGGING'):
            logging_config = settings.LOGGING
            handlers = logging_config.get('handlers', {})
            
            print("ğŸ“‹ Djangoæ—¥å¿—å¤„ç†å™¨:")
            for handler_name, handler_config in handlers.items():
                handler_class = handler_config.get('class', 'unknown')
                print(f"   - {handler_name}: {handler_class}")
        
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("   1. å®ç°Redis Streamåˆ°MySQLçš„å®šæ—¶åŒæ­¥ä»»åŠ¡")
        print("   2. åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºunified_logsè¡¨ç»“æ„")
        print("   3. æ·»åŠ æ—¥å¿—ä¸­é—´ä»¶è‡ªåŠ¨å­˜å‚¨åˆ°æ•°æ®åº“")
    
    def check_alternative_log_tables(self):
        """æ£€æŸ¥å…¶ä»–å¯èƒ½çš„æ—¥å¿—è¡¨"""
        print("\nğŸ” æ£€æŸ¥å…¶ä»–æ—¥å¿—ç›¸å…³è¡¨...")
        
        try:
            with connection.cursor() as cursor:
                # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«logçš„è¡¨
                cursor.execute("SHOW TABLES")
                all_tables = [table[0] for table in cursor.fetchall()]
                log_tables = [table for table in all_tables if 'log' in table.lower()]
                
                print(f"ğŸ“‹ åŒ…å«'log'çš„è¡¨: {log_tables}")
                
                # æ£€æŸ¥æ‰§è¡Œç›¸å…³çš„æ—¥å¿—è¡¨
                execution_tables = [table for table in all_tables if 'execution' in table.lower()]
                print(f"ğŸ“‹ åŒ…å«'execution'çš„è¡¨: {execution_tables}")
                
                # å¦‚æœæœ‰æ‰§è¡Œè¡¨ï¼Œæ£€æŸ¥å…¶ä¸­æ˜¯å¦æœ‰æ—¥å¿—å­—æ®µ
                for table in execution_tables[:3]:  # åªæ£€æŸ¥å‰3ä¸ª
                    cursor.execute(f"DESCRIBE {table}")
                    columns = cursor.fetchall()
                    log_columns = [col[0] for col in columns if 'log' in col[0].lower()]
                    if log_columns:
                        print(f"   - {table} æœ‰æ—¥å¿—å­—æ®µ: {log_columns}")
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å…¶ä»–æ—¥å¿—è¡¨å¤±è´¥: {e}")
    
    def analyze_log_indexing(self):
        """åˆ†ææ—¥å¿—ç´¢å¼•ç³»ç»Ÿ"""
        print("\nğŸ” åˆ†ææ—¥å¿—ç´¢å¼•ç³»ç»Ÿ...")
        
        # æ£€æŸ¥ElasticSearchæ˜¯å¦å¯ç”¨
        print("ğŸ“‹ ElasticSearchçŠ¶æ€: æœªå¯ç”¨ (åœ¨å½“å‰é…ç½®ä¸­)")
        print("ğŸ“‹ å½“å‰ä½¿ç”¨MySQLä½œä¸ºæœç´¢åå¤‡")
        
        # æ£€æŸ¥æœç´¢æ€§èƒ½
        try:
            with connection.cursor() as cursor:
                cursor.execute("""
                    SELECT COUNT(*) FROM information_schema.statistics 
                    WHERE table_schema = 'ansflow' 
                    AND table_name = 'unified_logs'
                """)
                index_count = cursor.fetchone()[0] if cursor.fetchone() else 0
                print(f"ğŸ“Š unified_logsè¡¨ç´¢å¼•æ•°é‡: {index_count}")
        except:
            print("âš ï¸  æ— æ³•æ£€æŸ¥unified_logsè¡¨ç´¢å¼•")
    
    def generate_consistency_report(self):
        """ç”Ÿæˆæ•°æ®ä¸€è‡´æ€§æŠ¥å‘Š"""
        print("\nğŸ“Š æ•°æ®ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        
        print("\nğŸ¯ å‘ç°çš„ä¸»è¦é—®é¢˜:")
        print("1. å®æ—¶æ—¥å¿—ç¼ºå¤±FastAPIæœåŠ¡æ•°æ®")
        print("   - FastAPIæ—¥å¿—æ²¡æœ‰å†™å…¥Redis Stream")
        print("   - åªèƒ½çœ‹åˆ°Djangoå’ŒSystemæ—¥å¿—")
        
        print("\n2. å†å²æŸ¥è¯¢æ•°æ®é‡æœ‰é™")
        print("   - åªæœ‰24æ¡è®°å½•ï¼Œç–‘ä¼¼æµ‹è¯•æ•°æ®")
        print("   - ç¼ºå°‘ç”Ÿäº§ç¯å¢ƒçš„çœŸå®æ—¥å¿—æ•°æ®")
        
        print("\n3. æ•°æ®æµæ°´çº¿ä¸å®Œæ•´")
        print("   - Redis Stream â†’ MySQL ç¼ºå°‘è‡ªåŠ¨åŒæ­¥")
        print("   - åº”ç”¨æ—¥å¿—æ²¡æœ‰ç»Ÿä¸€å†™å…¥å†å²æ•°æ®åº“")
        
        print("\nğŸ’¡ æ¨èçš„è§£å†³æ–¹æ¡ˆ:")
        print("1. ä¿®å¤FastAPIæ—¥å¿—é›†æˆ:")
        print("   - åœ¨FastAPIä¸­å®ç°DirectRedisHandler")
        print("   - ç¡®ä¿æ‰€æœ‰åº”ç”¨æ—¥å¿—éƒ½å†™å…¥Redis Stream")
        
        print("\n2. å®ç°å†å²æ—¥å¿—è‡ªåŠ¨å­˜å‚¨:")
        print("   - åˆ›å»ºRedis Stream â†’ MySQLåŒæ­¥ä»»åŠ¡")
        print("   - å®ç°æ—¥å¿—ä¸­é—´ä»¶è‡ªåŠ¨å­˜å‚¨")
        
        print("\n3. å®Œå–„æ•°æ®ä¸€è‡´æ€§ä¿è¯:")
        print("   - ä½¿ç”¨UnifiedLogConnectoråŒæ­¥å†™å…¥")
        print("   - æ·»åŠ æ•°æ®ä¸€è‡´æ€§éªŒè¯æœºåˆ¶")
    
    def run_diagnosis(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("ğŸš€ å¼€å§‹AnsFlowæ—¥å¿—ç³»ç»Ÿæ•°æ®ä¸€è‡´æ€§è¯Šæ–­...")
        print("=" * 60)
        
        self.analyze_realtime_logs()
        self.analyze_historical_logs()
        self.analyze_log_indexing()
        self.generate_consistency_report()
        
        print("\nâœ… è¯Šæ–­å®Œæˆ!")


if __name__ == "__main__":
    diagnostic = LogConsistencyDiagnostic()
    diagnostic.run_diagnosis()

#!/usr/bin/env python3
"""
AnsFlowæ—¥å¿—ç³»ç»ŸPhase 3ç‹¬ç«‹æµ‹è¯•è„šæœ¬
ä¸ä¾èµ–Djangoç¯å¢ƒçš„ç®€åŒ–æµ‹è¯•
"""

import os
import sys
import json
import gzip
import re
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any, Optional


class StandaloneLogFileIndexer:
    """ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶ç´¢å¼•å™¨ï¼ˆä¸ä¾èµ–Djangoï¼‰"""
    
    def __init__(self, log_dir: str = None):
        self.log_dir = log_dir or "/Users/creed/Workspace/OpenSource/ansflow/logs"
        self.cache = {}
    
    def build_file_index(self, days: int = 30) -> Dict[str, Any]:
        """æ„å»ºæ–‡ä»¶ç´¢å¼•"""
        try:
            log_path = Path(self.log_dir)
            if not log_path.exists():
                return {
                    'files': [],
                    'date_range': [],
                    'services': [],
                    'levels': [],
                    'total_size': 0,
                    'build_time': datetime.now().isoformat()
                }
            
            cutoff_date = datetime.now() - timedelta(days=days)
            files = []
            services = set()
            levels = set()
            total_size = 0
            
            for file_path in log_path.glob("**/*.log"):
                if file_path.is_file():
                    stat = file_path.stat()
                    modified = datetime.fromtimestamp(stat.st_mtime)
                    
                    if modified >= cutoff_date:
                        service = self._extract_service_from_path(str(file_path))
                        level = self._extract_level_from_path(str(file_path))
                        
                        files.append({
                            'path': str(file_path),
                            'relative_path': str(file_path.relative_to(log_path)),
                            'size': stat.st_size,
                            'modified': modified.isoformat(),
                            'is_compressed': file_path.suffix == '.gz',
                            'service': service,
                            'level': level
                        })
                        
                        services.add(service)
                        levels.add(level)
                        total_size += stat.st_size
            
            # æ£€æŸ¥å‹ç¼©æ–‡ä»¶
            for file_path in log_path.glob("**/*.log.gz"):
                if file_path.is_file():
                    stat = file_path.stat()
                    modified = datetime.fromtimestamp(stat.st_mtime)
                    
                    if modified >= cutoff_date:
                        service = self._extract_service_from_path(str(file_path))
                        level = self._extract_level_from_path(str(file_path))
                        
                        files.append({
                            'path': str(file_path),
                            'relative_path': str(file_path.relative_to(log_path)),
                            'size': stat.st_size,
                            'modified': modified.isoformat(),
                            'is_compressed': True,
                            'service': service,
                            'level': level
                        })
                        
                        services.add(service)
                        levels.add(level)
                        total_size += stat.st_size
            
            return {
                'files': sorted(files, key=lambda x: x['modified'], reverse=True),
                'date_range': [
                    cutoff_date.strftime('%Y-%m-%d'),
                    datetime.now().strftime('%Y-%m-%d')
                ],
                'services': sorted(list(services)),
                'levels': sorted(list(levels)),
                'total_size': total_size,
                'build_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"æ„å»ºæ–‡ä»¶ç´¢å¼•å¤±è´¥: {e}")
            return {
                'files': [],
                'date_range': [],
                'services': [],
                'levels': [],
                'total_size': 0,
                'build_time': datetime.now().isoformat(),
                'error': str(e)
            }
    
    def _extract_service_from_path(self, path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–æœåŠ¡å"""
        if 'django' in path.lower():
            return 'django'
        elif 'fastapi' in path.lower():
            return 'fastapi'
        elif 'system' in path.lower():
            return 'system'
        elif 'test' in path.lower():
            return 'test'
        else:
            return 'unknown'
    
    def _extract_level_from_path(self, path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–æ—¥å¿—çº§åˆ«"""
        path_lower = path.lower()
        if 'error' in path_lower:
            return 'ERROR'
        elif 'warning' in path_lower:
            return 'WARNING'
        elif 'debug' in path_lower:
            return 'DEBUG'
        else:
            return 'INFO'


class StandaloneLogQueryEngine:
    """ç‹¬ç«‹çš„æ—¥å¿—æŸ¥è¯¢å¼•æ“ï¼ˆä¸ä¾èµ–Djangoï¼‰"""
    
    def __init__(self, log_dir: str = None):
        self.log_dir = log_dir or "/Users/creed/Workspace/OpenSource/ansflow/logs"
        self.indexer = StandaloneLogFileIndexer(log_dir)
    
    def search_logs(self, query_params: Dict[str, Any]) -> Dict[str, Any]:
        """æœç´¢æ—¥å¿—"""
        try:
            # è·å–ç›¸å…³æ–‡ä»¶
            relevant_files = self._get_relevant_files(query_params)
            
            if not relevant_files:
                return {
                    'logs': [],
                    'total_count': 0,
                    'files_searched': 0,
                    'query_time': datetime.now().isoformat(),
                    'has_more': False
                }
            
            all_logs = []
            files_searched = 0
            
            for file_info in relevant_files:
                try:
                    logs = self._search_in_file(file_info['path'], query_params)
                    all_logs.extend(logs)
                    files_searched += 1
                    
                    # é™åˆ¶ç»“æœæ•°é‡ä»¥æé«˜æ€§èƒ½
                    if len(all_logs) > query_params.get('limit', 1000):
                        break
                        
                except Exception as e:
                    print(f"æœç´¢æ–‡ä»¶ {file_info['path']} å¤±è´¥: {e}")
                    continue
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            all_logs.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
            
            # åˆ†é¡µå¤„ç†
            limit = query_params.get('limit', 100)
            offset = query_params.get('offset', 0)
            
            paginated_logs = all_logs[offset:offset + limit]
            
            return {
                'logs': paginated_logs,
                'total_count': len(all_logs),
                'files_searched': files_searched,
                'query_time': datetime.now().isoformat(),
                'has_more': len(all_logs) > offset + limit
            }
            
        except Exception as e:
            print(f"æ—¥å¿—æœç´¢å¤±è´¥: {e}")
            return {
                'logs': [],
                'total_count': 0,
                'files_searched': 0,
                'query_time': datetime.now().isoformat(),
                'has_more': False,
                'error': str(e)
            }
    
    def _get_relevant_files(self, query_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–ç›¸å…³æ–‡ä»¶åˆ—è¡¨"""
        index = self.indexer.build_file_index()
        files = index['files']
        
        # è¿‡æ»¤æ¡ä»¶
        start_time = query_params.get('start_time')
        end_time = query_params.get('end_time')
        services = query_params.get('services', [])
        levels = query_params.get('levels', [])
        
        filtered_files = []
        
        for file_info in files:
            # æ—¶é—´è¿‡æ»¤
            if start_time:
                try:
                    file_time = datetime.fromisoformat(file_info['modified'])
                    query_start = datetime.fromisoformat(start_time)
                    if file_time < query_start:
                        continue
                except:
                    pass
            
            if end_time:
                try:
                    file_time = datetime.fromisoformat(file_info['modified'])
                    query_end = datetime.fromisoformat(end_time)
                    if file_time > query_end:
                        continue
                except:
                    pass
            
            # æœåŠ¡è¿‡æ»¤
            if services and file_info['service'] not in services:
                continue
            
            # çº§åˆ«è¿‡æ»¤
            if levels and file_info['level'] not in levels:
                continue
            
            filtered_files.append(file_info)
        
        return filtered_files
    
    def _search_in_file(self, file_path: str, query_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åœ¨å•ä¸ªæ–‡ä»¶ä¸­æœç´¢"""
        logs = []
        keywords = query_params.get('keywords', '')
        
        try:
            # åˆ¤æ–­æ˜¯å¦ä¸ºå‹ç¼©æ–‡ä»¶
            if file_path.endswith('.gz'):
                file_obj = gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore')
            else:
                file_obj = open(file_path, 'r', encoding='utf-8', errors='ignore')
            
            with file_obj:
                for line_num, line in enumerate(file_obj, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # å…³é”®å­—è¿‡æ»¤
                    if keywords:
                        keyword_list = [k.strip() for k in keywords.split() if k.strip()]
                        if not any(keyword.lower() in line.lower() for keyword in keyword_list):
                            continue
                    
                    # è§£ææ—¥å¿—è¡Œ
                    log_entry = self._parse_log_line(line, file_path, line_num)
                    if log_entry:
                        logs.append(log_entry)
                        
                        # é™åˆ¶å•æ–‡ä»¶ç»“æœæ•°é‡
                        if len(logs) >= 500:
                            break
            
            return logs
            
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return []
    
    def _parse_log_line(self, line: str, file_path: str, line_num: int) -> Optional[Dict[str, Any]]:
        """è§£ææ—¥å¿—è¡Œ"""
        try:
            # å°è¯•è§£æJSONæ ¼å¼æ—¥å¿—
            if line.startswith('{'):
                try:
                    data = json.loads(line)
                    return {
                        'timestamp': data.get('timestamp', ''),
                        'level': data.get('level', 'INFO'),
                        'service': data.get('service', 'unknown'),
                        'logger': data.get('logger', ''),
                        'message': data.get('message', line),
                        'file': file_path,
                        'line_number': line_num,
                        'raw': False
                    }
                except json.JSONDecodeError:
                    pass
            
            # å°è¯•è§£æä¼ ç»Ÿæ ¼å¼ [æ—¶é—´] çº§åˆ« æ¨¡å—: æ¶ˆæ¯
            pattern = r'^(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}[.,]\d+).*?(\w+)\s+(.+?):\s*(.+)$'
            match = re.match(pattern, line)
            
            if match:
                timestamp, level, logger, message = match.groups()
                return {
                    'timestamp': timestamp.replace(',', '.'),
                    'level': level.upper(),
                    'service': self._extract_service_from_path(file_path),
                    'logger': logger.strip(),
                    'message': message.strip(),
                    'file': file_path,
                    'line_number': line_num,
                    'raw': False
                }
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸå§‹æ ¼å¼
            return {
                'timestamp': datetime.now().isoformat(),
                'level': 'INFO',
                'service': self._extract_service_from_path(file_path),
                'logger': 'unknown',
                'message': line,
                'file': file_path,
                'line_number': line_num,
                'raw': True
            }
            
        except Exception as e:
            return None
    
    def _extract_service_from_path(self, path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–æœåŠ¡å"""
        if 'django' in path.lower():
            return 'django'
        elif 'fastapi' in path.lower():
            return 'fastapi'
        elif 'system' in path.lower():
            return 'system'
        elif 'test' in path.lower():
            return 'test'
        else:
            return 'unknown'


class StandaloneLogAnalyzer:
    """ç‹¬ç«‹çš„æ—¥å¿—åˆ†æå™¨ï¼ˆä¸ä¾èµ–Djangoï¼‰"""
    
    def __init__(self, log_dir: str = None):
        self.log_dir = log_dir or "/Users/creed/Workspace/OpenSource/ansflow/logs"
        self.query_engine = StandaloneLogQueryEngine(log_dir)
    
    def analyze_trends(self, days: int = 7) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—è¶‹åŠ¿"""
        try:
            # æŸ¥è¯¢æœ€è¿‘Nå¤©çš„æ‰€æœ‰æ—¥å¿—
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            
            query_params = {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'limit': 10000  # å¢åŠ é™åˆ¶ä»¥è·å–æ›´å¤šæ•°æ®
            }
            
            result = self.query_engine.search_logs(query_params)
            logs = result['logs']
            
            if not logs:
                return {
                    'time_range': {
                        'start': start_time.isoformat(),
                        'end': end_time.isoformat(),
                        'days': days
                    },
                    'summary': {
                        'total_logs': 0,
                        'files_analyzed': 0
                    },
                    'by_level': {},
                    'by_service': {},
                    'by_hour': {},
                    'by_date': {},
                    'error_patterns': [],
                    'top_loggers': {},
                    'generated_at': datetime.now().isoformat()
                }
            
            # ç»Ÿè®¡åˆ†æ
            by_level = Counter()
            by_service = Counter()
            by_hour = defaultdict(int)
            by_date = defaultdict(int)
            top_loggers = Counter()
            error_patterns = []
            
            for log in logs:
                # çº§åˆ«ç»Ÿè®¡
                by_level[log.get('level', 'INFO')] += 1
                
                # æœåŠ¡ç»Ÿè®¡
                by_service[log.get('service', 'unknown')] += 1
                
                # Loggerç»Ÿè®¡
                logger_name = log.get('logger', 'unknown')
                top_loggers[logger_name] += 1
                
                # æ—¶é—´ç»Ÿè®¡
                timestamp = log.get('timestamp', '')
                if timestamp:
                    try:
                        dt = self._parse_timestamp(timestamp)
                        if dt:
                            hour_key = dt.strftime('%H:00')
                            date_key = dt.strftime('%Y-%m-%d')
                            by_hour[hour_key] += 1
                            by_date[date_key] += 1
                    except:
                        pass
                
                # é”™è¯¯æ¨¡å¼æ”¶é›†
                if log.get('level') in ['ERROR', 'CRITICAL']:
                    error_patterns.append({
                        'message': log.get('message', '')[:200],  # é™åˆ¶é•¿åº¦
                        'service': log.get('service', 'unknown'),
                        'timestamp': log.get('timestamp', ''),
                        'logger': log.get('logger', 'unknown')
                    })
            
            return {
                'time_range': {
                    'start': start_time.isoformat(),
                    'end': end_time.isoformat(),
                    'days': days
                },
                'summary': {
                    'total_logs': len(logs),
                    'files_analyzed': result['files_searched']
                },
                'by_level': dict(by_level),
                'by_service': dict(by_service),
                'by_hour': dict(by_hour),
                'by_date': dict(by_date),
                'error_patterns': error_patterns[:20],  # åªè¿”å›å‰20ä¸ªé”™è¯¯
                'top_loggers': dict(top_loggers.most_common(10)),
                'generated_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"æ—¥å¿—åˆ†æå¤±è´¥: {e}")
            return {
                'time_range': {
                    'start': start_time.isoformat() if 'start_time' in locals() else '',
                    'end': end_time.isoformat() if 'end_time' in locals() else '',
                    'days': days
                },
                'summary': {
                    'total_logs': 0,
                    'files_analyzed': 0
                },
                'by_level': {},
                'by_service': {},
                'error': str(e),
                'generated_at': datetime.now().isoformat()
            }
    
    def _parse_timestamp(self, timestamp_str: str) -> Optional[datetime]:
        """è§£ææ—¶é—´æˆ³"""
        formats = [
            '%Y-%m-%dT%H:%M:%S.%f',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%d %H:%M:%S.%f',
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M:%S,%f'
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(timestamp_str[:26], fmt)
            except ValueError:
                continue
        
        return None


def create_sample_logs():
    """åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ•°æ®"""
    print("=" * 60)
    print("ğŸ“ åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ•°æ®")
    print("=" * 60)
    
    log_dir = Path("/Users/creed/Workspace/OpenSource/ansflow/logs")
    log_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ—¥å¿—æ–‡ä»¶
    sample_logs = [
        {
            'filename': 'django_test.log',
            'service': 'django',
            'entries': [
                '2025-08-05T15:45:00.123 INFO django.request: GET /api/settings/logging/',
                '2025-08-05T15:45:01.456 WARNING django.security: Invalid authentication attempt',
                '2025-08-05T15:45:02.789 ERROR django.db: Database connection failed',
                '2025-08-05T15:45:03.012 INFO django.middleware: Request processed successfully'
            ]
        },
        {
            'filename': 'fastapi_test.log',
            'service': 'fastapi',
            'entries': [
                '{"timestamp": "2025-08-05T15:45:00.123", "level": "INFO", "service": "fastapi", "logger": "fastapi.main", "message": "Application startup complete"}',
                '{"timestamp": "2025-08-05T15:45:01.456", "level": "WARNING", "service": "fastapi", "logger": "fastapi.middleware", "message": "Slow request detected"}',
                '{"timestamp": "2025-08-05T15:45:02.789", "level": "ERROR", "service": "fastapi", "logger": "fastapi.routes", "message": "Route handler exception"}',
                '{"timestamp": "2025-08-05T15:45:03.012", "level": "DEBUG", "service": "fastapi", "logger": "fastapi.websocket", "message": "WebSocket connection established"}'
            ]
        },
        {
            'filename': 'system_test.log',
            'service': 'system',
            'entries': [
                '2025-08-05 15:45:00,123 INFO system.monitor: System health check passed',
                '2025-08-05 15:45:01,456 WARNING system.disk: Disk usage above 80%',
                '2025-08-05 15:45:02,789 CRITICAL system.memory: Memory usage critical',
                '2025-08-05 15:45:03,012 INFO system.process: Process cleanup completed'
            ]
        }
    ]
    
    created_files = []
    
    for log_file in sample_logs:
        file_path = log_dir / log_file['filename']
        
        with open(file_path, 'w', encoding='utf-8') as f:
            for entry in log_file['entries']:
                f.write(entry + '\n')
        
        created_files.append({
            'path': str(file_path),
            'service': log_file['service'],
            'entries': len(log_file['entries'])
        })
    
    print("âœ… ç¤ºä¾‹æ—¥å¿—åˆ›å»ºæˆåŠŸ")
    for file_info in created_files:
        print(f"   - {file_info['path']} ({file_info['service']}, {file_info['entries']} entries)")
    
    return created_files


def test_file_indexer():
    """æµ‹è¯•æ–‡ä»¶ç´¢å¼•åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ“ æµ‹è¯•æ—¥å¿—æ–‡ä»¶ç´¢å¼•åŠŸèƒ½")
    print("=" * 60)
    
    try:
        indexer = StandaloneLogFileIndexer()
        index = indexer.build_file_index(days=7)
        
        print(f"âœ… æ–‡ä»¶ç´¢å¼•æ„å»ºæˆåŠŸ")
        print(f"   - æ–‡ä»¶æ•°é‡: {len(index['files'])}")
        print(f"   - æ€»å¤§å°: {round(index['total_size'] / 1024, 2)} KB")
        print(f"   - æœåŠ¡åˆ—è¡¨: {index['services']}")
        print(f"   - æ—¥å¿—çº§åˆ«: {index['levels']}")
        print(f"   - æ—¶é—´èŒƒå›´: {index['date_range']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ç´¢å¼•æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_query_engine():
    """æµ‹è¯•æŸ¥è¯¢å¼•æ“åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ” æµ‹è¯•æ—¥å¿—æŸ¥è¯¢å¼•æ“åŠŸèƒ½")
    print("=" * 60)
    
    try:
        engine = StandaloneLogQueryEngine()
        
        # æµ‹è¯•åŸºæœ¬æŸ¥è¯¢
        query_params = {
            'keywords': 'ERROR WARNING',
            'limit': 10,
            'offset': 0
        }
        
        result = engine.search_logs(query_params)
        
        print(f"âœ… æŸ¥è¯¢å¼•æ“æµ‹è¯•æˆåŠŸ")
        print(f"   - æœç´¢ç»“æœ: {result['total_count']} æ¡æ—¥å¿—")
        print(f"   - æœç´¢æ–‡ä»¶: {result['files_searched']} ä¸ªæ–‡ä»¶")
        print(f"   - æŸ¥è¯¢æ—¶é—´: {result['query_time']}")
        
        # æ˜¾ç¤ºå‰å‡ æ¡ç»“æœ
        for i, log in enumerate(result['logs'][:3], 1):
            print(f"   - æ—¥å¿—{i}: [{log['level']}] {log['message'][:50]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_analyzer():
    """æµ‹è¯•æ—¥å¿—åˆ†æåŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ“Š æµ‹è¯•æ—¥å¿—åˆ†æåŠŸèƒ½")
    print("=" * 60)
    
    try:
        analyzer = StandaloneLogAnalyzer()
        analysis = analyzer.analyze_trends(days=7)
        
        print(f"âœ… æ—¥å¿—åˆ†ææµ‹è¯•æˆåŠŸ")
        print(f"   - åˆ†ææ—¶é—´èŒƒå›´: {analysis['time_range']['days']} å¤©")
        print(f"   - æ€»æ—¥å¿—æ•°: {analysis['summary']['total_logs']}")
        print(f"   - åˆ†ææ–‡ä»¶æ•°: {analysis['summary']['files_analyzed']}")
        print(f"   - æŒ‰çº§åˆ«ç»Ÿè®¡: {analysis['by_level']}")
        print(f"   - æŒ‰æœåŠ¡ç»Ÿè®¡: {analysis['by_service']}")
        print(f"   - é”™è¯¯æ¨¡å¼æ•°: {len(analysis['error_patterns'])}")
        print(f"   - Top Loggeræ•°: {len(analysis['top_loggers'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ—¥å¿—åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        return False


def test_prometheus_metrics():
    """æµ‹è¯•PrometheusæŒ‡æ ‡åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ“ˆ æµ‹è¯•PrometheusæŒ‡æ ‡æ”¶é›†")
    print("=" * 60)
    
    try:
        # æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›†
        analyzer = StandaloneLogAnalyzer()
        indexer = StandaloneLogFileIndexer()
        
        analysis = analyzer.analyze_trends(days=1)
        index = indexer.build_file_index()
        
        # ç”ŸæˆæŒ‡æ ‡æ•°æ®
        metrics_data = {
            'log_messages_total': analysis['summary']['total_logs'],
            'log_files_total': len(index['files']),
            'log_storage_bytes': index['total_size'],
            'log_by_level': analysis['by_level'],
            'log_by_service': analysis['by_service']
        }
        
        # ç”ŸæˆPrometheusæ ¼å¼
        prometheus_lines = []
        prometheus_lines.append("# HELP ansflow_log_messages_total Total number of log messages")
        prometheus_lines.append("# TYPE ansflow_log_messages_total counter")
        prometheus_lines.append(f"ansflow_log_messages_total {metrics_data['log_messages_total']}")
        prometheus_lines.append("")
        
        prometheus_lines.append("# HELP ansflow_log_files_total Total number of log files")
        prometheus_lines.append("# TYPE ansflow_log_files_total gauge")
        prometheus_lines.append(f"ansflow_log_files_total {metrics_data['log_files_total']}")
        prometheus_lines.append("")
        
        prometheus_format = "\n".join(prometheus_lines)
        
        print(f"âœ… PrometheusæŒ‡æ ‡æµ‹è¯•æˆåŠŸ")
        print(f"   - æ€»æ—¥å¿—æ¶ˆæ¯æ•°: {metrics_data['log_messages_total']}")
        print(f"   - æ€»æ–‡ä»¶æ•°: {metrics_data['log_files_total']}")
        print(f"   - å­˜å‚¨å¤§å°: {metrics_data['log_storage_bytes']} bytes")
        print(f"   - æŒ‰çº§åˆ«ç»Ÿè®¡: {metrics_data['log_by_level']}")
        print(f"   - æŒ‰æœåŠ¡ç»Ÿè®¡: {metrics_data['log_by_service']}")
        print(f"   - Prometheusæ ¼å¼é•¿åº¦: {len(prometheus_format)} chars")
        
        return True
        
    except Exception as e:
        print(f"âŒ PrometheusæŒ‡æ ‡æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ AnsFlowæ—¥å¿—ç³»ç»ŸPhase 3ç‹¬ç«‹æµ‹è¯•")
    print("=" * 80)
    print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
    print("ğŸ—ï¸  å‡†å¤‡æµ‹è¯•ç¯å¢ƒ...")
    created_files = create_sample_logs()
    
    # è¿è¡Œæµ‹è¯•
    test_results = {}
    
    tests = [
        ("æ–‡ä»¶ç´¢å¼•å™¨", test_file_indexer),
        ("æŸ¥è¯¢å¼•æ“", test_query_engine),
        ("æ—¥å¿—åˆ†æå™¨", test_analyzer),
        ("PrometheusæŒ‡æ ‡", test_prometheus_metrics)
    ]
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª è¿è¡Œ {test_name} æµ‹è¯•...")
        success = test_func()
        test_results[test_name] = success
    
    # æµ‹è¯•æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š Phase 3ç‹¬ç«‹æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 80)
    
    passed_count = 0
    total_count = len(test_results)
    
    for test_name, success in test_results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:<20}: {status}")
        if success:
            passed_count += 1
    
    print("-" * 80)
    print(f"æ€»ä½“ç»“æœ: {passed_count}/{total_count} æµ‹è¯•é€šè¿‡")
    
    if passed_count == total_count:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Phase 3åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        return 1


if __name__ == "__main__":
    sys.exit(main())
